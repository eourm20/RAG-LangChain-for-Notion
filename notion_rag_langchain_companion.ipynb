{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c9e6f0",
   "metadata": {
    "id": "25c9e6f0"
   },
   "source": [
    "\n",
    "# Notion RAG × LangChain Companion (A/B/C Variants)\n",
    "\n",
    "**기존 Notion 기반 RAG 커스텀 코드의 0 ~ 6단계는 유지**하고, 아래 3가지 LangChain 도입\n",
    "\n",
    "- **A안**: 0 ~ 5은 기존 코드 그대로, **6, 7단계인 LLM 사용만 LangChain**으로 감싸기 (최소침습 어댑터)\n",
    "- **B안**: 0 ~ 4은 유지, **5~7을 LangChain**으로 대체 (실험/확장 속도 강화)\n",
    "- **C안**: **풀 LC** — 0 ~ 7 전체를 LangChain 생태계로 구성 (PoC/프로토타입에 유리)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef8f069",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34333,
     "status": "ok",
     "timestamp": 1757686746988,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "9ef8f069",
    "outputId": "6d23b896-a961-4a18-eae7-2cc2322d64c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: notion-client in /usr/local/lib/python3.12/dist-packages (2.5.0)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
      "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.33)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.0.21)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from notion-client) (0.28.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.24)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.106.1)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->notion-client) (4.10.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->notion-client) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->notion-client) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->notion-client) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->notion-client) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.56.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.8.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (선택) 필수 패키지 설치\n",
    "!pip install notion-client langchain langchain-community langchain-openai chromadb tiktoken\n",
    "!pip install sentence_transformers  # (선택) reranker 등 확장 시\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b851f05",
   "metadata": {
    "id": "4b851f05"
   },
   "source": [
    "\n",
    "## 가정(당신의 기존 노트북에 이미 존재하는 객체)\n",
    "\n",
    "이 노트북의 A/B 코드들은 다음 **기존 변수/객체**가 이미 준비되어 있다고 가정합니다.\n",
    "\n",
    "- `page_uuid`: 정규화된 Notion Page UUID (예: `\"26c77d75-e008-81d5-a6ee-e7034c46c949\"`)\n",
    "- `markdown_text` 혹은 `raw_text`: 페이지에서 추출된 최종 텍스트(청크 분할 전)\n",
    "- `chunks` 또는 `docs`: 청크 리스트 (문자열 혹은 LangChain Document 리스트 중 하나)\n",
    "- `emb`: 기존 임베딩 객체(예: `OpenAIEmbeddings` 유사 API) — `.embed_query(text)` 사용 가능\n",
    "- `collection`: 기존 Chroma(혹은 유사) 벡터 스토어 컬렉션 객체 — `.query(query_embeddings=[...], n_results=k)`\n",
    "\n",
    "> 만약 변수명이 다른 경우, 아래 코드의 해당 부분만 바꿔서 사용하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51363c94",
   "metadata": {
    "id": "51363c94"
   },
   "source": [
    "# 🗂️ 노션 데이터로 나만의 RAG 시스템 구축하기(기존 커스텀 RAG 코드)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f1766",
   "metadata": {
    "id": "242f1766"
   },
   "source": [
    "노션 API로 페이지/데이터베이스를 불러와 **임베딩 → FAISS 검색 → LLM 생성**까지 한 번에 테스트하기\n",
    "\n",
    "- **임베딩**: BAAI/bge-m3 (로컬)\n",
    "- **벡터 스토어**: FAISS (메모리)\n",
    "- **LLM**: OpenAI-호환 API (기본 OpenRouter; BASE_URL/MODEL_NAME 교체로 Groq/Together/Fireworks 사용 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ba7342",
   "metadata": {
    "executionInfo": {
     "elapsed": 3282,
     "status": "ok",
     "timestamp": 1757687088959,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "45ba7342"
   },
   "outputs": [],
   "source": [
    "# 0) Install deps\n",
    "!pip -q install notion-client sentence-transformers faiss-cpu openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c6b57",
   "metadata": {
    "id": "a02c6b57"
   },
   "source": [
    "## 0) 환경 설정\n",
    "- **Notion**: 내부 통합 생성 후 `NOTION_TOKEN`을 입력하세요.\n",
    "    - https://www.notion.so/profile/integrations\n",
    "- **LLM**: OpenRouter 기준(`PROVIDER_API_KEY` 필요). 다른 제공자는 BASE_URL/MODEL_NAME만 바꾸세요.\n",
    "    - https://openrouter.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edb4f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1757696011892,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "f0edb4f3",
    "outputId": "ae1aefb5-bd15-4f0c-fd1d-d9ec053794d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOTION_TOKEN': True, 'BASE_URL': 'https://openrouter.ai/api/v1', 'MODEL_NAME': 'meta-llama/llama-3.1-8b-instruct', 'EMB_MODEL': 'BAAI/bge-m3'}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# --- Notion ---\n",
    "NOTION_TOKEN = '' #'ntn_xxx'\n",
    "\n",
    "# --- LLM (OpenAI-호환) ---\n",
    "API_KEY = '' # 'sk-or-v1-xxx'\n",
    "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "MODEL_NAME = \"meta-llama/llama-3.1-8b-instruct\"\n",
    "\n",
    "# --- Embedding ---\n",
    "EMB_MODEL = \"BAAI/bge-m3\"\n",
    "\n",
    "print({\n",
    "    \"NOTION_TOKEN\": bool(NOTION_TOKEN),\n",
    "    \"BASE_URL\": BASE_URL,\n",
    "    \"MODEL_NAME\": MODEL_NAME,\n",
    "    \"EMB_MODEL\": EMB_MODEL,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b635a",
   "metadata": {
    "id": "df5b635a"
   },
   "source": [
    "## 1) Notion API 유틸 (페이지/DB → Markdown 텍스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "45256607",
   "metadata": {
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1757696926366,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "45256607"
   },
   "outputs": [],
   "source": [
    "from notion_client import Client\n",
    "import re, textwrap, hashlib\n",
    "from typing import List, Dict\n",
    "\n",
    "if not NOTION_TOKEN:\n",
    "    raise RuntimeError(\"NOTION_TOKEN이 필요합니다.\")\n",
    "nclient = Client(auth=NOTION_TOKEN)\n",
    "\n",
    "def _pt(rt_list):\n",
    "    return \"\".join([t.get(\"plain_text\",\"\") for t in (rt_list or [])])\n",
    "\n",
    "def _flatten_block(block):\n",
    "    t = block[\"type\"]\n",
    "    b = block[t]\n",
    "    if t == \"paragraph\":\n",
    "        return _pt(b.get(\"rich_text\"))\n",
    "    if t.endswith(\"_heading\"):\n",
    "        return \"# \" + _pt(b.get(\"rich_text\"))\n",
    "    if t in (\"bulleted_list_item\",\"numbered_list_item\",\"to_do\"):\n",
    "        return \"- \" + _pt(b.get(\"rich_text\"))\n",
    "    if t == \"quote\":\n",
    "        return \"> \" + _pt(b.get(\"rich_text\"))\n",
    "    if t == \"code\":\n",
    "        txt = b.get(\"rich_text\", [{}])[0].get(\"plain_text\",\"\")\n",
    "        lang = b.get(\"language\",\"\")\n",
    "        return f\"```{lang}\\n\"+txt+\"\\n```\"\n",
    "    if t == \"callout\":\n",
    "        return \"💡 \" + _pt(b.get(\"rich_text\"))\n",
    "    if t == \"toggle\":\n",
    "        return _pt(b.get(\"rich_text\"))  # children로 확장\n",
    "    if t == \"equation\":\n",
    "        return \"$\" + b.get(\"expression\",\"\") + \"$\"\n",
    "    if t == \"table_row\":\n",
    "        cells = [ _pt(cell) for cell in b.get(\"cells\", []) ]\n",
    "        return \" | \".join(cells)\n",
    "    return \"\"\n",
    "\n",
    "def _walk_children(block_id, acc: List[str]):\n",
    "    children = nclient.blocks.children.list(block_id=block_id)\n",
    "    while True:\n",
    "        for b in children[\"results\"]:\n",
    "            acc.append(_flatten_block(b))\n",
    "            if b.get(\"has_children\"):\n",
    "                _walk_children(b[\"id\"], acc)\n",
    "        if not children.get(\"has_more\"): break\n",
    "        children = nclient.blocks.children.list(block_id=block_id, start_cursor=children[\"next_cursor\"])\n",
    "\n",
    "# 페이지를 재귀로 순회해 텍스트화\n",
    "def notion_page_to_markdown(page_id: str) -> str:\n",
    "    out=[]\n",
    "    _walk_children(page_id, out)\n",
    "    md = \"\\n\".join(filter(None,out)).strip()\n",
    "    return md\n",
    "\n",
    "def get_page_meta(page: Dict) -> Dict:\n",
    "    # title property 찾기\n",
    "    props = page.get(\"properties\", {})\n",
    "    title_prop = next((v for v in props.values() if v.get(\"type\")==\"title\"), None)\n",
    "    title = _pt((title_prop or {}).get(\"title\", [])) or page.get(\"id\")\n",
    "    return {\n",
    "        \"page_id\": page[\"id\"],\n",
    "        \"title\": title,\n",
    "        \"url\": page.get(\"url\"),\n",
    "        \"last_edited_time\": page.get(\"last_edited_time\"),\n",
    "    }\n",
    "\n",
    "# DB의 각 페이지를 위 함수로 변환\n",
    "def fetch_pages_from_database(database_id: str) -> List[Dict]:\n",
    "    results=[]\n",
    "    resp = nclient.databases.query(database_id=database_id, page_size=50)\n",
    "    while True:\n",
    "        for page in resp[\"results\"]:\n",
    "            meta = get_page_meta(page)\n",
    "            md = notion_page_to_markdown(page[\"id\"])\n",
    "            results.append({**meta, \"content_md\": md})\n",
    "        if not resp.get(\"has_more\"): break\n",
    "        resp = nclient.databases.query(database_id=database_id, page_size=50, start_cursor=resp[\"next_cursor\"])\n",
    "    return results\n",
    "\n",
    "# 단일 페이지 변환\n",
    "def fetch_single_page(page_id: str) -> Dict:\n",
    "    page = nclient.pages.retrieve(page_id=page_id)\n",
    "    meta = get_page_meta(page)\n",
    "    md = notion_page_to_markdown(page_id)\n",
    "    return {**meta, \"content_md\": md}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3a694",
   "metadata": {
    "id": "beb3a694"
   },
   "source": [
    "## 2) 대상 선택: 데이터베이스 ID 또는 개별 페이지 ID\n",
    "- 원하는 `database_id`/`page_id`를 넣어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "55e4e535",
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1757696975658,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "55e4e535"
   },
   "outputs": [],
   "source": [
    "# 예시: 하나의 데이터베이스를 긁어오거나, 개별 페이지들을 모아올 수 있습니다.\n",
    "DATABASE_IDS = [\n",
    "    # \"264bf0ad3a0680e18fedda127323e553\",\n",
    "    # \"15c8cc2b57a44ae7901d48122862a22a\",\n",
    "]\n",
    "PAGE_IDS = [\n",
    "    \"24977d75e00880369b93e2653613eec3\",\n",
    "    # \"44ce9a98e0074495922c917ba96cf631\",\n",
    "    \"22e77d75e00880ea9085d1410be465fb\",\n",
    "    # \"15c8cc2b57a44ae7901d48122862a22a\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927440e0",
   "metadata": {
    "id": "927440e0"
   },
   "source": [
    "## 3) Notion → 문서 리스트 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ea3863c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7351,
     "status": "ok",
     "timestamp": 1757696983858,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "ea3863c5",
    "outputId": "10d8f618-5604-4014-c107-f93d46477d75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " [('스터디방향성', 'https://www.notion.so/24977d75e00880369b93e2653613eec3'),\n",
       "  ('계획', 'https://www.notion.so/22e77d75e00880ea9085d1410be465fb')])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for dbid in DATABASE_IDS:\n",
    "    docs += fetch_pages_from_database(dbid)\n",
    "\n",
    "for pid in PAGE_IDS:\n",
    "    docs.append(fetch_single_page(pid))\n",
    "\n",
    "len(docs), [ (d['title'], d['url']) for d in docs[:5] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ZxNlgm0o3Bx4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1757696986642,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "ZxNlgm0o3Bx4",
    "outputId": "4ac8eff2-d2a1-453c-a2a7-9103a8369e70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_id': '24977d75-e008-8036-9b93-e2653613eec3',\n",
       "  'title': '스터디방향성',\n",
       "  'url': 'https://www.notion.so/24977d75e00880369b93e2653613eec3',\n",
       "  'last_edited_time': '2025-08-24T04:13:00.000Z',\n",
       "  'content_md': '카테고리 | 많이 쓰는 도구 | 이유 / 현업 사용 포인트\\n데이터 버전관리 | DVC, Git LFS | DVC는 데이터 파이프라인 추적 + 원격 저장 연동에 강함, Git LFS는 단순 대용량 파일 버전 관리\\n파이프라인 관리 | Airflow, Prefect | Airflow는 대규모 기업·배치 작업 표준, Prefect는 파이썬 친화적이고 설정이 단순\\n실험 관리 | MLflow, wandb | MLflow는 오픈소스 표준, wandb는 시각화·협업 기능이 강하고 스타트업/연구실에서 인기\\n분산 학습 | PyTorch DDP, Ray | 대규모 학습은 DDP가 표준, Ray는 DDP 래핑 + Task 분산 둘 다 가능\\n모델 서빙 | FastAPI, BentoML | FastAPI는 가볍고 유연, BentoML은 모델 버전 관리 + 서빙 일체형\\n배포 | Docker, Kubernetes(K8s) | Docker는 개발·테스트 필수, 운영 환경 확장은 K8s가 사실상 표준\\n모니터링 | Prometheus + Grafana, Evidently AI | Prom+Graf는 시스템/모델 지표 모니터링 표준, Evidently는 데이터 드리프트·성능 모니터링 특화\\n- DHKim역량: 실험 관리, 분산학습, 배포, 모니터링 쪽 가능 \\n- 실험관리 (visualization): MLflow, wandb\\n- 분산학습: Ray, PyTorchDDP, Accelerate 등\\n- 배포: Docker\\n- 모니터링: Prometheus + Grafana (현재 연구실 GPU모니터링 시스템 만든이력있음)\\n- promtQL\\n새로배워야될 항목\\n- DVC, Airflow, Prefect, Evidently AI\\n영역 | 할 수 있는 것 | 대표 컴포넌트/키워드 | 예시 사용처\\n프롬프트 구성 | 템플릿·변수 바인딩·체이닝 | PromptTemplate, ChatPromptTemplate | 역할지시 + 사용자 입력 조합해서 일관된 프롬프트 생성\\n모델 호출 래핑 | 다양한 LLM/챗모델/임베딩 호출 | ChatOpenAI, ChatAnthropic, HuggingFaceEndpoints, OpenAIEmbeddings | OpenAI/Anthropic/로컬 LLM 등 통합 인터페이스\\n구조화된 출력 | JSON/스키마 강제, 파서 | PydanticOutputParser, StructuredOutputParser, 함수/툴 호출 | API 응답처럼 JSON 스키마 보장\\n체인 오케스트레이션 | 단계 연결, 조건 분기, 병렬/스트리밍 | LCEL(Runnable*), map, batch, stream | “질의→검색→요약” 파이프라인 한 줄로 구성\\n메모리 | 대화 히스토리/요약/키값 메모리 | ConversationBufferMemory, BufferWindow, SummaryMemory | 멀티턴 챗봇 컨텍스트 유지\\n문서 로딩 | 웹/파일/노션/구글드라이브 등 로더 | DocumentLoaders (PDF/HTML/Notion/GDrive/S3/Playwright 등) | 사내·외부 문서 읽어오기\\n텍스트 분할 | 길이 기반/의미 기반 스플릿 | RecursiveCharacterTextSplitter, Markdown/Text splitters | 긴 문서를 RAG 친화적으로 쪼개기\\n임베딩 & 벡터DB | 임베딩 생성·저장·검색 | FAISS, Chroma, Pinecone, Weaviate, Milvus | RAG 인덱스 구축\\n리트리버 (RAG) | 다양한 검색 전략 | VectorStoreRetriever, MultiQueryRetriever, ContextualCompressionRetriever, SelfQueryRetriever | 멀티쿼리·필터·요약 압축 검색\\nRerank/압축 | 컨텍스트 재랭킹·요약 압축 | ContextualCompression, LLMChainExtractor, (BM25 등 통합) | 노이즈 줄이고 정답 근거만 전달\\n쿼리 변환 | 질의 재작성/확장/하이픈 | QueryTransformation, HypotheticalDocumentEmbedding(HyDE) | 모模호가한 질문을 검색 친화적으로 변경\\n인덱스/그래프 | SQL/그래프/파일 혼합 질의 | SQLDatabaseChain, Cypher QA(Neo4j) | “자연어→SQL/사이퍼” 질의\\n에이전트 | 툴 사용·계획 실행 | AgentExecutor, Tool, ReAct, 함수/툴 호출 | 웹검색→코드실행→요약 같은 멀티스텝 자동화\\n툴 연동 | 코드/검색/DB/웹 등 | PythonREPLTool, RequestsTool/Browser, SQLTool, ShellTool | 코드 스니펫 실행, REST 호출, DB 질의\\n평가/Eval | 자동 평가·채점·맨션 | langchain.evaluation(QA/맞춤 평가자) | RAG 응답 정확도/근거 일치성 측정\\n콜백/로깅 | 토큰/지연시간/중간단계 로깅 | Callbacks, (LangSmith 연동) | 디버깅·트레이싱\\n캐시 | 모델 응답 캐싱 | InMemoryCache, Redis/SQLite Cache | 비용 절감·속도 향상\\n퍼시스턴스 | 체크포인트/상태 저장(경량) | Checkpoint, History | 긴 워크플로 재시작\\n프롬프트 공유 | 프롬프트·체인 레시피 공유 | LangChain Hub | 팀 내 표준 템플릿 재사용\\n- 잘 모르지만 이런것들을 할 수 있음\\n- 인원: MLOps 2명 / LangChain 2명\\n- 진행: 주 1회(2시간) \\n- 각 분야 1시간씩: 개념 설명(20분) + 실습 데모(30분) + Q&A(10분)\\n- 수업자료: Jupyter Notebook + 슬라이드 or 블로그 요약(선택) → GitHub 업로드\\n- 기간: 4~6개월\\n목표: 개념 잡기 + 환경 셋업 + 간단한 실습\\n주차 | MLOps팀 | LangChain팀\\n1주 | MLOps 개요, 구성요소 | LangChain 개요, 구성요소\\n2주 | Docker 기초 | LLMChain, PromptTemplate\\n3주 | DVC로 데이터 버전 관리 | DocumentLoader, TextSplitter\\n4주 | MLflow 실험 기록 | Embedding + VectorStore\\n5주 | 복습/보충 주차 | 복습/보충 주차\\n6주 | Prefect or Airflow 파이프라인 | Retriever & RAG 기본\\n목표: 개별 분야 심화, 실전 기능 구현\\n주차 | MLOps팀 | LangChain팀\\n7주 | Ray 분산 처리 기초 | MultiQueryRetriever, Context 압축\\n8주 | PyTorch DDP | Tool & Agent 기초\\n9주 | FastAPI 모델 서빙 | 외부 API 연동 Agent\\n10주 | Docker-compose 환경 구성 | RAG+Agent 결합\\n11주 | 복습/보충 주차 | 복습/보충 주차\\n12주 | Kubernetes 기초 | LangChain 메모리/캐시\\n목표: 양 팀 연결, 배포 가능 상태까지\\n주차 | MLOps팀 | LangChain팀\\n13주 | MLflow 모델 레지스트리 | 데이터 전처리 자동화\\n14주 | CI/CD (GitHub Actions) | LangChain 평가(evaluation)\\n15주 | 모니터링(Prom+Graf or Evidently) | API → UI 연동(Streamlit)\\n16주 | 통합 리허설 | 통합 리허설\\n17~18주* | 버퍼 주차 (예정치 못한 지연 대비) | 버퍼 주차\\n- 실습 최소 단위: Jupyter에서 30~50줄 내외 코드로 끝낼 수 있는 예제로 제한.\\n- 복습 주차 필수: 4~5주마다 한 번은 발표 없이 지난 내용 실습/코드 수정만.\\n- 평일: 하루 20~30분, 발표 준비나 코드 작성\\n- 주말: 모임(2시간) + 발표/질문\\n- 매주: 발표 끝나면 바로 GitHub 업로드 (정리 최소화, 코드+간단설명)\\n- 끝나고, 사이드 프로젝트로 Configuration 생성기 Agent 를 만들어보는건 어떨까?\\nMLOps는 보통 config.json이 굉장히 길어짐 (Model관련, metric관련, dataset관련, 각각 함수의 하이퍼파라미터들 다 입력해야하니까) 초창기 러닝커브가 심해짐\\n⇒ 따라서, LangChain기반 Agent를 우리가 구축한 간단한 MLOps 파이프라인의 config 파일을 유저의 query에 따라 맞춤형으로 json 이나 yaml파일을 생성해주는 chatbot을 만드는 것도 좋아보임\\n예를들면,\\n> 유저:\\n\"mnist 데이터로 ResNet+UNet 임베딩, ViT+MLP 분류를 각각 돌려보고 싶다.\"\\n> Agent 응답:\\n\"요청하신 구조에 맞춰 ResNet+UNet config와 ViT+MLP config를 생성했습니다.\\n경로: ./config/resnet_unet.yaml, ./config/vit_mlp.yaml\\n기본 하이퍼파라미터로 세팅했으며, GPU 1개 기준입니다.\"\\nMLOps: 간단한 자동화 파이프라인\\nLangChain: 유저 쿼리 처리 (기존 config파일 참고:Rag) 응답생성 → 템플릿기반 파일 생성 → 파일 저장 수행 (기능수행) 이런 흐름으로 도구화가 가능해보임\\n- 더하면 아예 MLOps 자동화 Agent일텐데.. 이건너무 빡세보임'},\n",
       " {'page_id': '22e77d75-e008-80ea-9085-d1410be465fb',\n",
       "  'title': '계획',\n",
       "  'url': 'https://www.notion.so/22e77d75e00880ea9085d1410be465fb',\n",
       "  'last_edited_time': '2025-08-24T04:06:00.000Z',\n",
       "  'content_md': 'Week 1~8\\nWeek 1 – 정렬 & 리스트 활용\\n핵심 개념: sorted(), sort(), lambda, 리스트 인덱스 조작\\n- K번째 수 (Lv.1)\\n- 문자열 내 마음대로 정렬하기 (Lv.1) \\n- 최댓값과 최솟값 (Lv.2)\\n- 최댓값 만들기 (1) (Lv.0)\\n- 문자열 정렬하기 (1) (Lv.0)\\nWeek 2 – 완전탐색 기본\\n핵심 개념: for문을 활용한 전체 탐색, 중첩 반복문 구조\\n- 모의고사 (Lv.1)\\n- 소수 찾기 (Lv.1)\\n- 삼총사 (Lv.1)\\n- 한 번만 등장한 문자 (Lv.0)\\n- 소수 만들기 (Lv.1)\\nWeek 3 – 조건 + 구현 + 시뮬레이션\\n핵심 개념: 조건 분기문, 값 누적, 반복문 흐름 설계\\n- 로또의 최고 순위와 최저 순위 (Lv.1)\\n- 신고 결과 받기 (Lv.1)\\n- 명예의 전당 (1) (Lv.1)\\n- 개인정보 수집 유효기간 (Lv.1~2)\\n- 햄버거 만들기 (Lv.1)\\nWeek 4: 해시 (Hash Table)\\n핵심 개념: 딕셔너리를 활용한 빠른 탐색, Key-Value 구조, dict, .get(), .items(), .setdefault()\\n- 완주하지 못한 선수 (Lv.1)\\n- 의상 (Lv.2)\\n- 폰켓몬 (Lv.1)\\n- 베스트앨범 (Lv.3)\\n- 성격 유형 검사하기 (Lv.1)\\nWeek 5: 그리디 (Greedy) \\n핵심 개념: 현재의 최적해가 전체 문제의 최적해가 되는 경우 탐색\\n- 체육복 (Lv.1)\\n- 큰 수 만들기 (Lv.2)\\n- 조이스틱 (Lv.2, 심화)\\n- 구명보트 (Lv.2)\\n- ATM (백준, S4) \\nWeek 6: 스택/큐\\n핵심 개념: 선입선출(FIFO), 후입선출(LIFO) 자료구조 활용, collections.deque, stack.append(), stack.pop()\\n- 같은 숫자는 싫어 (Lv.1)\\n- 올바른 괄호 (Lv.2)\\n- 기능개발 (Lv.2)\\n- 프린터 (Lv.2) 프로세스 (Lv.2)\\n- 괄호 회전하기 (Lv.2)\\nWeek 7: 이분탐색 (Binary Search)\\n핵심 개념: 정렬된 배열에서 효율적인 값 탐색\\n- 정수 제곱근 판별 (Lv.1)\\n- 예산 (Lv.1)\\n- 나무 자르기 (백준, S2)\\n- 입국심사 (Lv.3)\\n- 숫자 카드2 (백준, S4)\\nWeek 8: DFS / BFS (그래프 탐색 기본)\\n핵심 개념: 방문 체크를 통한 그래프 탐색, visited[], 재귀 함수, deque 활용\\n- 타겟 넘버 (DFS, Lv.2)\\n- 네트워크 (DFS/BFS, Lv.2)\\n- 단어 변환 (Lv.3)\\n- 게임 맵 최단거리 (BFS, Lv.2)\\n- 미로 탐색 (백준, S1)\\nWeek 9 – 동적 계획법 (DP) [동희]\\n핵심 개념: 점화식 정의, 메모이제이션 (dp[]) 1차원 vs 2차원 DP 구조 이해\\n- 피보나치 수 (Lv.2)\\n- 멀리 뛰기 (Lv.2)\\n- N으로 표현 (Lv.3)\\n- 정수 삼각형 (Lv.3)\\n- 등굣길 (Lv.3)\\n- 사칙연산 (Lv.4)\\n- 도둑질 (Lv.4)\\nWeek 10 – 최단 경로 & 다익스트라 (Dijkstra) [예린]\\n핵심 개념: 가중치 있는 그래프에서 최단 경로 탐색, distance[], heapq, 우선순위 큐\\n- 배달 (Lv.2)\\n- 등굣길 (Lv.3)\\n- 가장 먼 노드 (Lv.3) \\n- 합승 택시 요금 (Lv.3)\\n- 부대복귀 (Lv.3)\\n- 디스크 컨트롤러 (Lv.3) \\n- 숨바꼭질 3 (백준 13549)\\nWeek 11 – 백트래킹 & 유니온 파인드[수빈]\\n핵심 개념: 백트래킹: 완전탐색 + 조건 분기 최적화, 유니온 파인드 (find, union)\\n- N-Queen (Lv.3)\\n- 여행 경로 (Lv.3)\\n- 수식 최대화 (Lv.2)\\n- 섬 연결하기 (Lv.3) \\nWeek 12 –  모의고사\\n- 관심 분야 뭔지 이름 넣기~ 중복 상관없음~\\n- MLOps\\n- Ray (pytorch-multi processing 및 HPO 자동화)\\n- LangChain\\n- RAG, MCP\\n- LLM기반 AgentAI, Multi-Agent(숩)\\n- Foundation Model Tuning\\n- Fine tuning\\n- LoRA\\n- Transfer or Knowledge Distillation\\n- Multi-modal(숩)\\n- \\n- 8월 9일은 모의고사 푸는날~'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39bfac",
   "metadata": {
    "id": "9e39bfac"
   },
   "source": [
    "## 4) 전처리 & 청킹(Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4921e2cc",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1757696989472,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "4921e2cc"
   },
   "outputs": [],
   "source": [
    "def split_markdown(md: str, max_len=900):\n",
    "    parts=[]; buf=[]\n",
    "    for line in md.splitlines():\n",
    "        if re.match(r\"^#{1,6}\\s\", line) and buf:\n",
    "            chunk=\"\\n\".join(buf).strip()\n",
    "            parts += textwrap.wrap(chunk, max_len, break_long_words=False, break_on_hyphens=False) if len(chunk)>max_len else [chunk]\n",
    "            buf=[line]\n",
    "        else:\n",
    "            buf.append(line)\n",
    "    if buf:\n",
    "        chunk=\"\\n\".join(buf).strip()\n",
    "        parts += textwrap.wrap(chunk, max_len, break_long_words=False, break_on_hyphens=False) if len(chunk)>max_len else [chunk]\n",
    "    return [p for p in parts if p.strip()]\n",
    "\n",
    "chunks=[]\n",
    "metas=[]\n",
    "for d in docs:\n",
    "    for ch in split_markdown(d[\"content_md\"]):\n",
    "        metas.append({\"page_id\": d[\"page_id\"], \"title\": d[\"title\"], \"url\": d.get(\"url\"), \"section\": \"\", \"text\": ch})\n",
    "        chunks.append(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62ceb2",
   "metadata": {
    "id": "8b62ceb2"
   },
   "source": [
    "## 5) 임베딩 & 벡터 인덱스(FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ad104804",
   "metadata": {
    "executionInfo": {
     "elapsed": 66293,
     "status": "ok",
     "timestamp": 1757697056971,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "ad104804"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "e_model = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "def embed(texts):\n",
    "    return e_model.encode(texts, normalize_embeddings=True, convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "vecs = embed(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dbb5e9a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1757697056997,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "dbb5e9a4",
    "outputId": "c05c4edc-761c-43ee-98e2-5483cd7fa350"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, faiss\n",
    "\n",
    "class FaissStore:\n",
    "    def __init__(self, dim):\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.meta = []\n",
    "    def add(self, vecs, metas):\n",
    "        self.index.add(vecs)    # 학습 불필요, 바로 추가\n",
    "        self.meta += metas\n",
    "    def search(self, qvec, k=5):\n",
    "        D,I = self.index.search(np.array([qvec]).astype(\"float32\"), k)  # 유사도 높은 상위 k개\n",
    "        out=[]\n",
    "        for rank, idx in enumerate(I[0]):\n",
    "            if idx == -1: continue\n",
    "            m = self.meta[idx]\n",
    "            out.append({\"text\": m[\"text\"], \"meta\": {k:v for k,v in m.items() if k!=\"text\"}, \"score\": float(D[0][rank])})\n",
    "        return out\n",
    "\n",
    "store = FaissStore(vecs.shape[1])\n",
    "store.add(vecs, metas)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130bcdd2",
   "metadata": {
    "id": "130bcdd2"
   },
   "source": [
    "## 6) LLM 호출 (OpenAI-호환) 및 질의 → 검색 → 답변\n",
    "- 기본 OpenRouter, 필요시 BASE_URL/MODEL_NAME 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "75e81177",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2588,
     "status": "ok",
     "timestamp": 1757700822498,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "75e81177",
    "outputId": "cc575d73-bfd5-4948-d523-d522a6a0f8c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[답변]\n",
      " Week 3 계획은 다음과 같습니다.\n",
      "\n",
      "- 로또의 최고 순위와 최저 순위 (Lv.1)\n",
      "- 신고 결과 받기 (Lv.1)\n",
      "- 명예의 전당 (1) (Lv.1)\n",
      "- 개인정보 수집 유효기간 (Lv.1~2)\n",
      "- 햄버거 만들기 (Lv.1)\n",
      "\n",
      "이 계획은 조건 + 구현 + 시뮬레이션 핵심 개념을 학습하는 주차입니다.\n",
      "\n",
      "[근거]\n",
      "(1) 계획 | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(2) 계획 | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(3) 계획 | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(4) 스터디방향성 | https://www.notion.so/24977d75e00880369b93e2653613eec3\n",
      "(5) 스터디방향성 | https://www.notion.so/24977d75e00880369b93e2653613eec3\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"PROVIDER_API_KEY가 필요합니다.\")\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "SYSTEM = \"당신은 신뢰 가능한 한국어 어시스턴트입니다. 제공된 근거 외 추측 금지.\"\n",
    "\n",
    "def build_prompt(query, contexts):\n",
    "    ctx = \"\\n\\n---\\n\\n\".join(\n",
    "        f\"[{i+1}] {c['meta'].get('title','(제목없음)')} / {c['meta'].get('section','')}\\n{c['text']}\"\n",
    "        for i,c in enumerate(contexts)\n",
    "    )\n",
    "    return f\"\"\"사용자 질문: {query}\n",
    "\n",
    "다음 근거를 바탕으로 한국어로 정확히 답하세요.\n",
    "근거:\n",
    "{ctx}\n",
    "\n",
    "규칙:\n",
    "- 근거에 없는 내용은 '근거 없음'으로 표시\n",
    "- 필요한 경우 목록/표로 간결히\n",
    "- 각 주장에는 근거 번호를 붙여라\n",
    "\"\"\"\n",
    "\n",
    "def llm_answer(query, contexts, temperature=0.2, max_tokens=800):\n",
    "    prompt = build_prompt(query, contexts)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\":\"system\",\"content\":SYSTEM}, {\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "\n",
    "def embed_one(text):\n",
    "    return embed([text])[0]\n",
    "\n",
    "def ask(q: str, k: int = 8, n_ctx: int = 5):\n",
    "    qv = embed_one(q)\n",
    "    cands = store.search(qv, k=k)\n",
    "    contexts = cands[:n_ctx]\n",
    "    answer = llm_answer(q, contexts)\n",
    "    print(\"\\n[답변]\\n\", answer)\n",
    "    print(\"\\n[근거]\")\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        print(f\"({i}) {c['meta']['title']} | {c['meta'].get('url','')}\")\n",
    "    return answer, contexts\n",
    "\n",
    "# 예시 실행:\n",
    "answer, ctx = ask(\"Week 3 계획\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba047b8",
   "metadata": {
    "id": "aba047b8"
   },
   "source": [
    "\n",
    "---\n",
    "# A안: 6단계만 LangChain으로 감싸기 (최소한만 LangChain으로 변환)\n",
    "\n",
    "- 0)~5)까지는 **기존 코드 그대로 사용**\n",
    "- 6) **프롬프트→LLM** 부분만 LangChain 체인으로 선언적으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0670aec8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4239,
     "status": "ok",
     "timestamp": 1757700955525,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "0670aec8",
    "outputId": "0e3763cc-68da-42e6-f2ba-0023faae264a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[답변]\n",
      " Week 3 계획에 대한 정보는 다음과 같습니다.\n",
      "\n",
      "- Week 3 – 조건 + 구현 + 시뮬레이션 핵심 개념: 조건 분기문, 값 누적, 반복문 흐름 설계\n",
      "- 핵심 개념에 포함된 문제는 다음과 같습니다.\n",
      "  - 로또의 최고 순위와 최저 순위 (Lv.1)\n",
      "  - 신고 결과 받기 (Lv.1)\n",
      "  - 명예의 전당 (1) (Lv.1)\n",
      "  - 개인정보 수집 유효기간 (Lv.1~2)\n",
      "  - 햄버거 만들기 (Lv.1)\n",
      "\n",
      "[근거]\n",
      "(1) 계획 | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(2) 계획 | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(3) 계획 | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(4) 스터디방향성 | https://www.notion.so/24977d75e00880369b93e2653613eec3\n",
      "(5) 스터디방향성 | https://www.notion.so/24977d75e00880369b93e2653613eec3\n"
     ]
    }
   ],
   "source": [
    "# --- LangChain 버전: lanchain을 사용하기 위한 구조로 변형하는 작업 필요!! ---\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "# StrOutputParser 경로 호환\n",
    "try:\n",
    "    from langchain.schema.output_parser import StrOutputParser\n",
    "except ImportError:\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "_prompt_LC = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    (\"user\",\n",
    "     \"사용자 질문: {query}\\n\\n\"\n",
    "     \"다음 근거를 바탕으로 한국어로 정확히 답하세요.\\n\"\n",
    "     \"근거:\\n{contexts}\\n\\n\"\n",
    "     \"규칙:\\n\"\n",
    "     \"- 근거에 없는 내용은 '근거 없음'으로 표시\\n\"\n",
    "     \"- 필요한 경우 목록/표로 간결히\\n\"\n",
    "     \"- 각 주장에는 근거 번호를 붙여라\")\n",
    "])\n",
    "\n",
    "\n",
    "# LangChain 호환 가능 OpenAI 불러오기 ** LangChain 사용 가능한 모델?만 가능함 / 제약이 있는편(그래도 대부분 모델 가능)\n",
    "# LLM 정의 (기존 client와 동일한 설정)\n",
    "try:\n",
    "    LC_llm = ChatOpenAI(\n",
    "        model=MODEL_NAME,\n",
    "        api_key=API_KEY,\n",
    "        base_url=BASE_URL,\n",
    "    )\n",
    "except TypeError:  # 버전 호환\n",
    "    LC_llm = ChatOpenAI(\n",
    "        model=MODEL_NAME,\n",
    "        openai_api_key=API_KEY,\n",
    "        openai_api_base=BASE_URL,\n",
    "    )\n",
    "\n",
    "# 체인: build_prompt → LLM → 파서\n",
    "chain_A = (\n",
    "    RunnableLambda(lambda x: build_prompt(x[\"query\"], x[\"contexts\"]))\n",
    "    | LC_llm\n",
    "    | StrOutputParser() # 문자열만 그대로 Output내는 함수\n",
    ")\n",
    "\n",
    "def _format_contexts(contexts):\n",
    "    blocks = []\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        meta = c.get(\"meta\", {}) or {}\n",
    "        title = meta.get(\"title\", \"(제목없음)\")\n",
    "        section = meta.get(\"section\", \"\")\n",
    "        text = c.get(\"text\", \"\")\n",
    "        blocks.append(f\"[{i}] {title} / {section}\\n{text}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "# 함수: ask_A에서 쓸 수 있게 래핑\n",
    "def llm_answer_A(query, contexts, temperature=0.2, max_tokens=800):\n",
    "    llm_bound = LC_llm.bind(temperature=temperature,\n",
    "                           max_tokens=max_tokens,\n",
    "                           max_completion_tokens=max_tokens)\n",
    "    # 검색을 위한 프롬프트 생성부터 LLM 모델로 문장 생성까지 Chain으로 연결\n",
    "    chain = (\n",
    "        {  # 입력 매핑\n",
    "            \"query\": RunnablePassthrough(),\n",
    "            \"contexts\": RunnableLambda(lambda x: _format_contexts(x[\"contexts\"]))\n",
    "        }\n",
    "        | _prompt_LC      # ← 여기서 system/user 메시지로 변환\n",
    "        | llm_bound\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke({\"query\": query, \"contexts\": contexts})\n",
    "\n",
    "def embed_one(text):\n",
    "    return embed([text])[0]\n",
    "\n",
    "def ask_A(q: str, k: int = 8, n_ctx: int = 5):\n",
    "    qv = embed_one(q)\n",
    "    cands = store.search(qv, k=k) # 검색 단계는 기존 함수 유지\n",
    "    contexts = cands[:n_ctx]\n",
    "    answer = llm_answer_A(q, contexts)\n",
    "    print(\"\\n[답변]\\n\", answer)\n",
    "    print(\"\\n[근거]\")\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        print(f\"({i}) {c['meta']['title']} | {c['meta'].get('url','')}\")\n",
    "    return answer, contexts\n",
    "\n",
    "# 예시 실행:\n",
    "answer, ctx = ask_A(\"Week 3 계획\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NaqyfURmjm96",
   "metadata": {
    "id": "NaqyfURmjm96"
   },
   "source": [
    "\n",
    "---\n",
    "# B안: 5~6단계 LangChain 교체\n",
    "\n",
    "- 5)~6) **임베딩/벡터인덱스 + QA 체인(A안 6 단계)** 부분을 LangChain 체인으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "nd_eocalj5En",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74728,
     "status": "ok",
     "timestamp": 1757701672001,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "nd_eocalj5En",
    "outputId": "a316612b-c502-4bf8-d697-5aaa05477998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[답변]\n",
      " Week 3 계획입니다.\n",
      "\n",
      "[근거]\n",
      "(1) 계획 | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(2) 스터디방향성 | https://www.notion.so/24977d75e00880369b93e2653613eec3\n"
     ]
    }
   ],
   "source": [
    "# --- 호환 임포트 ---\n",
    "try:\n",
    "    from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "except ImportError:\n",
    "    from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "try:\n",
    "    from langchain.schema.output_parser import StrOutputParser\n",
    "except ImportError:\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- 벡터스토어 / 임베딩 래퍼 ---\n",
    "try:\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "except ImportError:\n",
    "    from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class LegacyEmbeddings(Embeddings):\n",
    "    def __init__(self, embed_fn): self.embed_fn = embed_fn\n",
    "    def embed_documents(self, texts): return self.embed_fn(texts)\n",
    "    def embed_query(self, text): return self.embed_fn([text])[0]\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.docstore.document import Document as LCDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "# ===== 0) SYSTEM 분리 프롬프트 =====\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "_prompt_LC = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    (\"user\",\n",
    "     \"사용자 질문: {query}\\n\\n\"\n",
    "     \"다음 근거를 바탕으로 한국어로 정확히 답하세요.\\n\"\n",
    "     \"근거:\\n{contexts}\\n\\n\"\n",
    "     \"규칙:\\n\"\n",
    "     \"- 근거에 없는 내용은 '근거 없음'으로 표시\\n\"\n",
    "     \"- 필요한 경우 목록/표로 간결히\\n\"\n",
    "     \"- 각 주장에는 근거 번호를 붙여라\")\n",
    "])\n",
    "\n",
    "# LLM 초기화 (기존 모델/키/base_url 그대로)\n",
    "try:\n",
    "    LC_llm = ChatOpenAI(model=MODEL_NAME, api_key=API_KEY, base_url=BASE_URL)\n",
    "except TypeError:\n",
    "    LC_llm = ChatOpenAI(model=MODEL_NAME, openai_api_key=API_KEY, openai_api_base=BASE_URL)\n",
    "\n",
    "# ===== 1) 제목 보완(헤더 추정) =====\n",
    "def _safe_title(meta: dict, text: str):\n",
    "    t = (meta or {}).get(\"title\")\n",
    "    if isinstance(t, str) and t.strip():\n",
    "        return t.strip()\n",
    "    m = re.search(r\"^\\s*#{1,6}\\s+(.+)$\", text, flags=re.MULTILINE)\n",
    "    return (m.group(1).strip() if m else \"(제목없음)\")\n",
    "\n",
    "# ===== 2) docs(LangChain Document or Notion dict) → LangChain Document 표준화 =====\n",
    "# FAISS 등 LangChain의 VectorStore는 반드시 Document 타입을 입력으로 받음\n",
    "def _to_documents(docs_or_chunks):\n",
    "    docs = []\n",
    "    for item in docs_or_chunks:\n",
    "        if isinstance(item, LCDocument):\n",
    "            meta = dict(item.metadata or {})\n",
    "            if \"title\" not in meta:\n",
    "                meta[\"title\"] = _safe_title(meta, item.page_content)\n",
    "            if \"url\" not in meta:\n",
    "                pid = meta.get(\"page_id\") or meta.get(\"id\")\n",
    "                if isinstance(pid, str) and pid:\n",
    "                    meta[\"url\"] = f\"https://www.notion.so/{pid.replace('-','')}\"\n",
    "            docs.append(Document(page_content=item.page_content, metadata=meta))\n",
    "        elif isinstance(item, dict):\n",
    "            text = item.get(\"content_md\") or item.get(\"text\") or \"\"\n",
    "            meta = {\n",
    "                \"title\": item.get(\"title\") or \"\",\n",
    "                \"url\": item.get(\"url\") or \"\",\n",
    "                \"page_id\": item.get(\"page_id\") or item.get(\"id\"),\n",
    "                \"last_edited_time\": item.get(\"last_edited_time\"),\n",
    "            }\n",
    "            if not meta[\"title\"]:\n",
    "                meta[\"title\"] = _safe_title(meta, text)\n",
    "            if not meta[\"url\"] and meta.get(\"page_id\"):\n",
    "                meta[\"url\"] = f\"https://www.notion.so/{meta['page_id'].replace('-','')}\"\n",
    "            docs.append(Document(page_content=text, metadata=meta))\n",
    "        else:\n",
    "            docs.append(Document(page_content=str(item), metadata={}))\n",
    "    return docs\n",
    "\n",
    "# ===== 3) 이미 있는 docs로 바로 retriever =====\n",
    "def build_retriever_B_from_docs(docs_ready, k: int = 8):\n",
    "    emb = LegacyEmbeddings(embed)   # 기존 embed([...]) 재사용\n",
    "    docs_std = _to_documents(docs_ready)\n",
    "    vs = FAISS.from_documents(docs_std, emb)\n",
    "    return vs.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# ===== 4) LangChain Document → 기존 컨텍스트(dict) 포맷 + title/url 보장 =====\n",
    "def _contexts_from_docs(docs):\n",
    "    ctxs = []\n",
    "    for d in docs:\n",
    "        meta = dict(d.metadata or {})\n",
    "        if \"title\" not in meta or not meta[\"title\"]:\n",
    "            meta[\"title\"] = _safe_title(meta, d.page_content)\n",
    "        if \"url\" not in meta or not meta[\"url\"]:\n",
    "            pid = meta.get(\"page_id\") or meta.get(\"id\")\n",
    "            if isinstance(pid, str) and pid:\n",
    "                meta[\"url\"] = f\"https://www.notion.so/{pid.replace('-','')}\"\n",
    "        ctxs.append({\"text\": d.page_content, \"meta\": meta})\n",
    "    return ctxs\n",
    "\n",
    "# ===== 5) contexts(dict 리스트) → 프롬프트용 문자열 =====\n",
    "def _format_contexts_for_prompt(contexts):\n",
    "    blocks = []\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        meta = c.get(\"meta\", {}) or {}\n",
    "        title = meta.get(\"title\", \"(제목없음)\")\n",
    "        section = meta.get(\"section\", \"\")\n",
    "        text = c.get(\"text\", \"\")\n",
    "        blocks.append(f\"[{i}] {title} / {section}\\n{text}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "# ===== 6) 체인 빌더: retriever → (query,contexts_str) → _prompt_LC → LLM → Parser =====\n",
    "def llm_answer_B(retriever, *, temperature: float = 0.2, max_tokens: int = 800):\n",
    "    llm_bound = LC_llm.bind(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        max_completion_tokens=max_tokens\n",
    "    )\n",
    "    chain = (\n",
    "        {\"query\": RunnablePassthrough(), \"docs\": retriever}   # 질의 → 검색\n",
    "        | RunnableLambda(lambda x: {\n",
    "            \"query\": x[\"query\"],\n",
    "            \"contexts\": _format_contexts_for_prompt(_contexts_from_docs(x[\"docs\"]))\n",
    "        })\n",
    "        | _prompt_LC\n",
    "        | llm_bound\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "# ===== 7) 실행 헬퍼 =====\n",
    "def ask_B(q: str, chain, retriever, n_ctx: int = 5):\n",
    "    answer = chain.invoke({\"query\": q})\n",
    "    docs_top = retriever.get_relevant_documents(q)[:n_ctx]\n",
    "    contexts = _contexts_from_docs(docs_top)\n",
    "\n",
    "    print(\"\\n[답변]\\n\", answer)\n",
    "    print(\"\\n[근거]\")\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        print(f\"({i}) {c['meta'].get('title','(제목없음)')} | {c['meta'].get('url','')}\")\n",
    "    return answer, contexts\n",
    "\n",
    "# ===== 8) 사용 예시 =====\n",
    "retrieverB = build_retriever_B_from_docs(docs, k=8)  # docs: 당신이 가진 Notion dict 리스트\n",
    "chainB = llm_answer_B(retrieverB, temperature=0.2, max_tokens=800)\n",
    "question = \"Week 3 계획\"\n",
    "answer, ctx = ask_B(question, chainB, retrieverB, n_ctx=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J8bOWW3iEJZo",
   "metadata": {
    "id": "J8bOWW3iEJZo"
   },
   "source": [
    "\n",
    "---\n",
    "# C안: Full LangChain 파이프라인\n",
    "\n",
    "- 데이터 로딩(~docs 생성까지 유지) -> 청킹 -> 임베딩/인덱스 -> 검색(리트리버) -> QA 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ARGuIn-FTMqP",
   "metadata": {
    "id": "ARGuIn-FTMqP"
   },
   "outputs": [],
   "source": [
    "# LangChain의 Notion 데이터 가져오는 라이브러리가 있는데 데이터베이스만 가능,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "Ly2l53zazLe3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61683,
     "status": "ok",
     "timestamp": 1757704278070,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "Ly2l53zazLe3",
    "outputId": "20e2c285-a502-488a-8a69-96bfe23d4281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[답변]\n",
      " Week 3 계획입니다.\n",
      "\n",
      "Week 3에서는 조건 + 구현 + 시뮬레이션을 주제로 공부합니다. 핵심 개념은 조건 분기문, 값 누적, 반복문 흐름 설계입니다.\n",
      "\n",
      "주어진 문제는 다음과 같습니다.\n",
      "\n",
      "- 로또의 최고 순위와 최저 순위 (Lv.1)\n",
      "- 신고 결과 받기 (Lv.1)\n",
      "- 명예의 전당 (1) (Lv.1)\n",
      "- 개인정보 수집 유효기간 (Lv.1~2)\n",
      "- 햄버거 만들기 (Lv.1)\n",
      "\n",
      "이 문제들은 조건 분기문, 값 누적, 반복문 흐름 설계를 사용하여 해결할 수 있습니다.\n",
      "\n",
      "[근거]\n",
      "(1) 계획 | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(2) 스터디방향성 | https://www.notion.so/24977d75e00880369b93e2653613eec3\n",
      "(3) 계획 | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(4) 스터디방향성 | https://www.notion.so/24977d75e00880369b93e2653613eec3\n",
      "(5) 스터디방향성 | https://www.notion.so/24977d75e00880369b93e2653613eec3\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# C안: Full LangChain 파이프라인\n",
    "# 로딩(선택) → 청킹 → 임베딩/인덱스 → Retriever → SYSTEM 분리 프롬프트 → LLM → 파서\n",
    "# ===============================\n",
    "\n",
    "# ---- 호환 임포트 (버전별 경로 차이 처리) ----\n",
    "try:\n",
    "    from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "except ImportError:\n",
    "    from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "try:\n",
    "    from langchain.schema.output_parser import StrOutputParser\n",
    "except ImportError:\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "try:\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "except ImportError:\n",
    "    from langchain.embeddings.base import Embeddings\n",
    "\n",
    "import re, uuid\n",
    "from typing import List, Iterable, Dict, Any\n",
    "\n",
    "# ---- LLM 초기화 (기존 설정 그대로) ----\n",
    "try:\n",
    "    LC_llm = ChatOpenAI(model=MODEL_NAME, api_key=API_KEY, base_url=BASE_URL)\n",
    "except TypeError:\n",
    "    LC_llm = ChatOpenAI(model=MODEL_NAME, openai_api_key=API_KEY, openai_api_base=BASE_URL)\n",
    "\n",
    "# ---- SYSTEM 분리 프롬프트 (기존 규칙 유지) ----\n",
    "_prompt_C = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    (\"user\",\n",
    "     \"사용자 질문: {query}\\n\\n\"\n",
    "     \"다음 근거를 바탕으로 한국어로 정확히 답하세요.\\n\"\n",
    "     \"근거:\\n{contexts}\\n\\n\"\n",
    "     \"규칙:\\n\"\n",
    "     \"- 근거에 없는 내용은 '근거 없음'으로 표시\\n\"\n",
    "     \"- 필요한 경우 목록/표로 간결히\\n\"\n",
    "     \"- 각 주장에는 근거 번호를 붙여라\")\n",
    "])\n",
    "\n",
    "# ---- 기존 embed([...])를 LangChain Embeddings로 래핑 ----\n",
    "class LegacyEmbeddings(Embeddings):\n",
    "    def __init__(self, embed_fn): self.embed_fn = embed_fn\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.embed_fn(texts)\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_fn([text])[0]\n",
    "\n",
    "# ---- 제목 보완 (헤더에서 추정) ----\n",
    "def _safe_title(meta: dict, text: str):\n",
    "    t = (meta or {}).get(\"title\")\n",
    "    if isinstance(t, str) and t.strip():\n",
    "        return t.strip()\n",
    "    m = re.search(r\"^\\s*#{1,6}\\s+(.+)$\", text, flags=re.MULTILINE)\n",
    "    return (m.group(1).strip() if m else \"(제목없음)\")\n",
    "\n",
    "# ---- Notion dict → LangChain Document 변환 (docs가 이미 있으면 이걸 사용) ----\n",
    "def docs_from_notion_dicts(items: List[dict]) -> List[Document]:\n",
    "    out = []\n",
    "    for it in items:\n",
    "        text = it.get(\"content_md\") or it.get(\"text\") or \"\"\n",
    "        meta = {\n",
    "            \"title\": it.get(\"title\") or \"\",\n",
    "            \"url\": it.get(\"url\") or \"\",\n",
    "            \"page_id\": it.get(\"page_id\") or it.get(\"id\"),\n",
    "            \"last_edited_time\": it.get(\"last_edited_time\"),\n",
    "            \"section\": it.get(\"section\",\"\"),\n",
    "        }\n",
    "        if not meta[\"title\"]:\n",
    "            meta[\"title\"] = _safe_title(meta, text)\n",
    "        if not meta[\"url\"] and isinstance(meta.get(\"page_id\"), str) and meta[\"page_id\"]:\n",
    "            meta[\"url\"] = f\"https://www.notion.so/{meta['page_id'].replace('-','')}\"\n",
    "        out.append(Document(page_content=text, metadata=meta))\n",
    "    return out\n",
    "\n",
    "# ---- (선택) Notion DB에서 직접 로딩하려면 사용 ----\n",
    "# from langchain_community.document_loaders import NotionDBLoader\n",
    "# def _normalize_notion_id(raw: str, with_hyphens: bool = True) -> str:\n",
    "#     hex32 = \"\".join(re.findall(r\"[0-9a-fA-F]\", raw)).lower()\n",
    "#     if len(hex32) != 32: raise ValueError(f\"Notion ID 길이 오류: {raw}\")\n",
    "#     return str(uuid.UUID(hex32)) if with_hyphens else hex32\n",
    "# def load_notion_db_docs(database_id: str, notion_token: str) -> List[Document]:\n",
    "#     db_id = _normalize_notion_id(database_id, with_hyphens=True)\n",
    "#     loader = NotionDBLoader(integration_token=notion_token, database_id=db_id)\n",
    "#     docs_loaded = loader.load()\n",
    "#     out = []\n",
    "#     for d in docs_loaded:\n",
    "#         meta = dict(d.metadata or {})\n",
    "#         if \"title\" not in meta or not meta[\"title\"]:\n",
    "#             m = re.search(r\"^\\s*#{1,6}\\s+(.+)$\", d.page_content, flags=re.MULTILINE)\n",
    "#             meta[\"title\"] = (m.group(1).strip() if m else \"(제목없음)\")\n",
    "#         if \"url\" not in meta or not meta[\"url\"]:\n",
    "#             pid = meta.get(\"page_id\") or meta.get(\"id\")\n",
    "#             if isinstance(pid, str) and pid:\n",
    "#                 meta[\"url\"] = f\"https://www.notion.so/{pid.replace('-','')}\"\n",
    "#         out.append(Document(page_content=d.page_content, metadata=meta))\n",
    "#     return out\n",
    "\n",
    "# ---- 청킹 ----\n",
    "def chunk_documents(docs: List[Document], chunk_size: int = 1000, chunk_overlap: int = 150) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# ---- 인덱싱/리트리버 (FAISS) ----\n",
    "def build_vs_and_retriever_C(chunked_docs: List[Document],\n",
    "                             embed_like=None,\n",
    "                             k: int = 8,\n",
    "                             search_type: str = \"mmr\",\n",
    "                             search_kwargs: Dict[str, Any] = None):\n",
    "    embeddings = embed_like or LegacyEmbeddings(embed)  # embed([...]) 사용\n",
    "    vs = FAISS.from_documents(chunked_docs, embeddings)\n",
    "    if search_kwargs is None:\n",
    "        search_kwargs = {\"k\": k, \"fetch_k\": 32, \"lambda_mult\": 0.5} if search_type==\"mmr\" else {\"k\": k}\n",
    "    retriever = vs.as_retriever(search_type=search_type, search_kwargs=search_kwargs)\n",
    "    return vs, retriever\n",
    "\n",
    "# ---- 컨텍스트 변환 (LLM 프롬프트용 문자열) ----\n",
    "def _contexts_from_docs(docs: Iterable[Document]):\n",
    "    ctxs = []\n",
    "    for d in docs:\n",
    "        meta = dict(d.metadata or {})\n",
    "        if \"title\" not in meta or not meta[\"title\"]:\n",
    "            meta[\"title\"] = _safe_title(meta, d.page_content)\n",
    "        if \"url\" not in meta or not meta[\"url\"]:\n",
    "            pid = meta.get(\"page_id\") or meta.get(\"id\")\n",
    "            if isinstance(pid, str) and pid:\n",
    "                meta[\"url\"] = f\"https://www.notion.so/{pid.replace('-','')}\"\n",
    "        ctxs.append({\"text\": d.page_content, \"meta\": meta})\n",
    "    return ctxs\n",
    "\n",
    "def _format_contexts_for_prompt(contexts: List[Dict[str, Any]]) -> str:\n",
    "    blocks = []\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        meta = c.get(\"meta\", {}) or {}\n",
    "        title = meta.get(\"title\", \"(제목없음)\")\n",
    "        section = meta.get(\"section\", \"\")\n",
    "        text = c.get(\"text\", \"\")\n",
    "        blocks.append(f\"[{i}] {title} / {section}\\n{text}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "# ---- QA 체인 (Retriever → Prompt → LLM → Parser) ----\n",
    "def build_chain_C(retriever, *, temperature: float = 0.2, max_tokens: int = 800):\n",
    "    llm_bound = LC_llm.bind(temperature=temperature, max_tokens=max_tokens, max_completion_tokens=max_tokens)\n",
    "    chain = (\n",
    "        {\"query\": RunnablePassthrough(), \"docs\": retriever}\n",
    "        | RunnableLambda(lambda x: {\n",
    "            \"query\": x[\"query\"],\n",
    "            \"contexts\": _format_contexts_for_prompt(_contexts_from_docs(x[\"docs\"]))\n",
    "        })\n",
    "        | _prompt_C\n",
    "        | llm_bound\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "def ask_C(question: str, chain, retriever, n_ctx: int = 5):\n",
    "    \"\"\"실행 + 상위 근거 출력 + 컨텍스트 반환\"\"\"\n",
    "    answer = chain.invoke({\"query\": question})\n",
    "    top_docs = retriever.get_relevant_documents(question)[:n_ctx]\n",
    "    contexts = _contexts_from_docs(top_docs)\n",
    "\n",
    "    print(\"\\n[답변]\\n\", answer)\n",
    "    print(\"\\n[근거]\")\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        print(f\"({i}) {c['meta'].get('title','(제목없음)')} | {c['meta'].get('url','')}\")\n",
    "    return answer, contexts\n",
    "\n",
    "# ===============================\n",
    "# 실행 예시\n",
    "# ===============================\n",
    "# (A) 이미 가지고 있는 docs(dict 리스트)를 사용\n",
    "docs_lc = docs_from_notion_dicts(docs)           # dict → LC Document\n",
    "chunked = chunk_documents(docs_lc, 1000, 150)    # 청킹\n",
    "vsC, retrieverC = build_vs_and_retriever_C(chunked, k=8, search_type=\"mmr\")\n",
    "chainC = build_chain_C(retrieverC, temperature=0.2, max_tokens=800)\n",
    "\n",
    "question = \"Week 3 계획\"\n",
    "answer, ctx = ask_C(question, chainC, retrieverC, n_ctx=5)\n",
    "\n",
    "# (B) 다른 임베딩 쓰고 싶으면:\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# emb_alt = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=API_KEY, base_url=BASE_URL)\n",
    "# vsC2, retrieverC2 = build_vs_and_retriever_C(chunked, embed_like=emb_alt, k=8, search_type=\"similarity\")\n",
    "# chainC2 = build_chain_C(retrieverC2)\n",
    "# answer2, ctx2 = ask_C(\"질문\", chainC2, retrieverC2)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
