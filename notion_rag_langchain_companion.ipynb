{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c9e6f0",
   "metadata": {
    "id": "25c9e6f0"
   },
   "source": [
    "\n",
    "# Notion RAG Ã— LangChain Companion (A/B/C Variants)\n",
    "\n",
    "**ê¸°ì¡´ Notion ê¸°ë°˜ RAG ì»¤ìŠ¤í…€ ì½”ë“œì˜ 0 ~ 6ë‹¨ê³„ëŠ” ìœ ì§€**í•˜ê³ , ì•„ë˜ 3ê°€ì§€ LangChain ë„ì…\n",
    "\n",
    "- **Aì•ˆ**: 0 ~ 5ì€ ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ, **6, 7ë‹¨ê³„ì¸ LLM ì‚¬ìš©ë§Œ LangChain**ìœ¼ë¡œ ê°ì‹¸ê¸° (ìµœì†Œì¹¨ìŠµ ì–´ëŒ‘í„°)\n",
    "- **Bì•ˆ**: 0 ~ 4ì€ ìœ ì§€, **5~7ì„ LangChain**ìœ¼ë¡œ ëŒ€ì²´ (ì‹¤í—˜/í™•ì¥ ì†ë„ ê°•í™”)\n",
    "- **Cì•ˆ**: **í’€ LC** â€” 0 ~ 7 ì „ì²´ë¥¼ LangChain ìƒíƒœê³„ë¡œ êµ¬ì„± (PoC/í”„ë¡œí† íƒ€ì…ì— ìœ ë¦¬)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef8f069",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34333,
     "status": "ok",
     "timestamp": 1757686746988,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "9ef8f069",
    "outputId": "6d23b896-a961-4a18-eae7-2cc2322d64c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: notion-client in /usr/local/lib/python3.12/dist-packages (2.5.0)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
      "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.33)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.0.21)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from notion-client) (0.28.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.24)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.106.1)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->notion-client) (4.10.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->notion-client) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->notion-client) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->notion-client) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->notion-client) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.56.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.8.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (ì„ íƒ) í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install notion-client langchain langchain-community langchain-openai chromadb tiktoken\n",
    "!pip install sentence_transformers  # (ì„ íƒ) reranker ë“± í™•ì¥ ì‹œ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b851f05",
   "metadata": {
    "id": "4b851f05"
   },
   "source": [
    "\n",
    "## ê°€ì •(ë‹¹ì‹ ì˜ ê¸°ì¡´ ë…¸íŠ¸ë¶ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê°ì²´)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì˜ A/B ì½”ë“œë“¤ì€ ë‹¤ìŒ **ê¸°ì¡´ ë³€ìˆ˜/ê°ì²´**ê°€ ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `page_uuid`: ì •ê·œí™”ëœ Notion Page UUID (ì˜ˆ: `\"26c77d75-e008-81d5-a6ee-e7034c46c949\"`)\n",
    "- `markdown_text` í˜¹ì€ `raw_text`: í˜ì´ì§€ì—ì„œ ì¶”ì¶œëœ ìµœì¢… í…ìŠ¤íŠ¸(ì²­í¬ ë¶„í•  ì „)\n",
    "- `chunks` ë˜ëŠ” `docs`: ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ë¬¸ìì—´ í˜¹ì€ LangChain Document ë¦¬ìŠ¤íŠ¸ ì¤‘ í•˜ë‚˜)\n",
    "- `emb`: ê¸°ì¡´ ì„ë² ë”© ê°ì²´(ì˜ˆ: `OpenAIEmbeddings` ìœ ì‚¬ API) â€” `.embed_query(text)` ì‚¬ìš© ê°€ëŠ¥\n",
    "- `collection`: ê¸°ì¡´ Chroma(í˜¹ì€ ìœ ì‚¬) ë²¡í„° ìŠ¤í† ì–´ ì»¬ë ‰ì…˜ ê°ì²´ â€” `.query(query_embeddings=[...], n_results=k)`\n",
    "\n",
    "> ë§Œì•½ ë³€ìˆ˜ëª…ì´ ë‹¤ë¥¸ ê²½ìš°, ì•„ë˜ ì½”ë“œì˜ í•´ë‹¹ ë¶€ë¶„ë§Œ ë°”ê¿”ì„œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51363c94",
   "metadata": {
    "id": "51363c94"
   },
   "source": [
    "# ğŸ—‚ï¸ ë…¸ì…˜ ë°ì´í„°ë¡œ ë‚˜ë§Œì˜ RAG ì‹œìŠ¤í…œ êµ¬ì¶•í•˜ê¸°(ê¸°ì¡´ ì»¤ìŠ¤í…€ RAG ì½”ë“œ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f1766",
   "metadata": {
    "id": "242f1766"
   },
   "source": [
    "ë…¸ì…˜ APIë¡œ í˜ì´ì§€/ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì™€ **ì„ë² ë”© â†’ FAISS ê²€ìƒ‰ â†’ LLM ìƒì„±**ê¹Œì§€ í•œ ë²ˆì— í…ŒìŠ¤íŠ¸í•˜ê¸°\n",
    "\n",
    "- **ì„ë² ë”©**: BAAI/bge-m3 (ë¡œì»¬)\n",
    "- **ë²¡í„° ìŠ¤í† ì–´**: FAISS (ë©”ëª¨ë¦¬)\n",
    "- **LLM**: OpenAI-í˜¸í™˜ API (ê¸°ë³¸ OpenRouter; BASE_URL/MODEL_NAME êµì²´ë¡œ Groq/Together/Fireworks ì‚¬ìš© ê°€ëŠ¥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ba7342",
   "metadata": {
    "executionInfo": {
     "elapsed": 3282,
     "status": "ok",
     "timestamp": 1757687088959,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "45ba7342"
   },
   "outputs": [],
   "source": [
    "# 0) Install deps\n",
    "!pip -q install notion-client sentence-transformers faiss-cpu openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c6b57",
   "metadata": {
    "id": "a02c6b57"
   },
   "source": [
    "## 0) í™˜ê²½ ì„¤ì •\n",
    "- **Notion**: ë‚´ë¶€ í†µí•© ìƒì„± í›„ `NOTION_TOKEN`ì„ ì…ë ¥í•˜ì„¸ìš”.\n",
    "    - https://www.notion.so/profile/integrations\n",
    "- **LLM**: OpenRouter ê¸°ì¤€(`PROVIDER_API_KEY` í•„ìš”). ë‹¤ë¥¸ ì œê³µìëŠ” BASE_URL/MODEL_NAMEë§Œ ë°”ê¾¸ì„¸ìš”.\n",
    "    - https://openrouter.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edb4f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1757696011892,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "f0edb4f3",
    "outputId": "ae1aefb5-bd15-4f0c-fd1d-d9ec053794d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOTION_TOKEN': True, 'BASE_URL': 'https://openrouter.ai/api/v1', 'MODEL_NAME': 'meta-llama/llama-3.1-8b-instruct', 'EMB_MODEL': 'BAAI/bge-m3'}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# --- Notion ---\n",
    "NOTION_TOKEN = '' #'ntn_xxx'\n",
    "\n",
    "# --- LLM (OpenAI-í˜¸í™˜) ---\n",
    "API_KEY = '' # 'sk-or-v1-xxx'\n",
    "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "MODEL_NAME = \"meta-llama/llama-3.1-8b-instruct\"\n",
    "\n",
    "# --- Embedding ---\n",
    "EMB_MODEL = \"BAAI/bge-m3\"\n",
    "\n",
    "print({\n",
    "    \"NOTION_TOKEN\": bool(NOTION_TOKEN),\n",
    "    \"BASE_URL\": BASE_URL,\n",
    "    \"MODEL_NAME\": MODEL_NAME,\n",
    "    \"EMB_MODEL\": EMB_MODEL,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b635a",
   "metadata": {
    "id": "df5b635a"
   },
   "source": [
    "## 1) Notion API ìœ í‹¸ (í˜ì´ì§€/DB â†’ Markdown í…ìŠ¤íŠ¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "45256607",
   "metadata": {
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1757696926366,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "45256607"
   },
   "outputs": [],
   "source": [
    "from notion_client import Client\n",
    "import re, textwrap, hashlib\n",
    "from typing import List, Dict\n",
    "\n",
    "if not NOTION_TOKEN:\n",
    "    raise RuntimeError(\"NOTION_TOKENì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "nclient = Client(auth=NOTION_TOKEN)\n",
    "\n",
    "def _pt(rt_list):\n",
    "    return \"\".join([t.get(\"plain_text\",\"\") for t in (rt_list or [])])\n",
    "\n",
    "def _flatten_block(block):\n",
    "    t = block[\"type\"]\n",
    "    b = block[t]\n",
    "    if t == \"paragraph\":\n",
    "        return _pt(b.get(\"rich_text\"))\n",
    "    if t.endswith(\"_heading\"):\n",
    "        return \"# \" + _pt(b.get(\"rich_text\"))\n",
    "    if t in (\"bulleted_list_item\",\"numbered_list_item\",\"to_do\"):\n",
    "        return \"- \" + _pt(b.get(\"rich_text\"))\n",
    "    if t == \"quote\":\n",
    "        return \"> \" + _pt(b.get(\"rich_text\"))\n",
    "    if t == \"code\":\n",
    "        txt = b.get(\"rich_text\", [{}])[0].get(\"plain_text\",\"\")\n",
    "        lang = b.get(\"language\",\"\")\n",
    "        return f\"```{lang}\\n\"+txt+\"\\n```\"\n",
    "    if t == \"callout\":\n",
    "        return \"ğŸ’¡ \" + _pt(b.get(\"rich_text\"))\n",
    "    if t == \"toggle\":\n",
    "        return _pt(b.get(\"rich_text\"))  # childrenë¡œ í™•ì¥\n",
    "    if t == \"equation\":\n",
    "        return \"$\" + b.get(\"expression\",\"\") + \"$\"\n",
    "    if t == \"table_row\":\n",
    "        cells = [ _pt(cell) for cell in b.get(\"cells\", []) ]\n",
    "        return \" | \".join(cells)\n",
    "    return \"\"\n",
    "\n",
    "def _walk_children(block_id, acc: List[str]):\n",
    "    children = nclient.blocks.children.list(block_id=block_id)\n",
    "    while True:\n",
    "        for b in children[\"results\"]:\n",
    "            acc.append(_flatten_block(b))\n",
    "            if b.get(\"has_children\"):\n",
    "                _walk_children(b[\"id\"], acc)\n",
    "        if not children.get(\"has_more\"): break\n",
    "        children = nclient.blocks.children.list(block_id=block_id, start_cursor=children[\"next_cursor\"])\n",
    "\n",
    "# í˜ì´ì§€ë¥¼ ì¬ê·€ë¡œ ìˆœíšŒí•´ í…ìŠ¤íŠ¸í™”\n",
    "def notion_page_to_markdown(page_id: str) -> str:\n",
    "    out=[]\n",
    "    _walk_children(page_id, out)\n",
    "    md = \"\\n\".join(filter(None,out)).strip()\n",
    "    return md\n",
    "\n",
    "def get_page_meta(page: Dict) -> Dict:\n",
    "    # title property ì°¾ê¸°\n",
    "    props = page.get(\"properties\", {})\n",
    "    title_prop = next((v for v in props.values() if v.get(\"type\")==\"title\"), None)\n",
    "    title = _pt((title_prop or {}).get(\"title\", [])) or page.get(\"id\")\n",
    "    return {\n",
    "        \"page_id\": page[\"id\"],\n",
    "        \"title\": title,\n",
    "        \"url\": page.get(\"url\"),\n",
    "        \"last_edited_time\": page.get(\"last_edited_time\"),\n",
    "    }\n",
    "\n",
    "# DBì˜ ê° í˜ì´ì§€ë¥¼ ìœ„ í•¨ìˆ˜ë¡œ ë³€í™˜\n",
    "def fetch_pages_from_database(database_id: str) -> List[Dict]:\n",
    "    results=[]\n",
    "    resp = nclient.databases.query(database_id=database_id, page_size=50)\n",
    "    while True:\n",
    "        for page in resp[\"results\"]:\n",
    "            meta = get_page_meta(page)\n",
    "            md = notion_page_to_markdown(page[\"id\"])\n",
    "            results.append({**meta, \"content_md\": md})\n",
    "        if not resp.get(\"has_more\"): break\n",
    "        resp = nclient.databases.query(database_id=database_id, page_size=50, start_cursor=resp[\"next_cursor\"])\n",
    "    return results\n",
    "\n",
    "# ë‹¨ì¼ í˜ì´ì§€ ë³€í™˜\n",
    "def fetch_single_page(page_id: str) -> Dict:\n",
    "    page = nclient.pages.retrieve(page_id=page_id)\n",
    "    meta = get_page_meta(page)\n",
    "    md = notion_page_to_markdown(page_id)\n",
    "    return {**meta, \"content_md\": md}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3a694",
   "metadata": {
    "id": "beb3a694"
   },
   "source": [
    "## 2) ëŒ€ìƒ ì„ íƒ: ë°ì´í„°ë² ì´ìŠ¤ ID ë˜ëŠ” ê°œë³„ í˜ì´ì§€ ID\n",
    "- ì›í•˜ëŠ” `database_id`/`page_id`ë¥¼ ë„£ì–´ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "55e4e535",
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1757696975658,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "55e4e535"
   },
   "outputs": [],
   "source": [
    "# ì˜ˆì‹œ: í•˜ë‚˜ì˜ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê¸ì–´ì˜¤ê±°ë‚˜, ê°œë³„ í˜ì´ì§€ë“¤ì„ ëª¨ì•„ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "DATABASE_IDS = [\n",
    "    # \"264bf0ad3a0680e18fedda127323e553\",\n",
    "    # \"15c8cc2b57a44ae7901d48122862a22a\",\n",
    "]\n",
    "PAGE_IDS = [\n",
    "    \"24977d75e00880369b93e2653613eec3\",\n",
    "    # \"44ce9a98e0074495922c917ba96cf631\",\n",
    "    \"22e77d75e00880ea9085d1410be465fb\",\n",
    "    # \"15c8cc2b57a44ae7901d48122862a22a\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927440e0",
   "metadata": {
    "id": "927440e0"
   },
   "source": [
    "## 3) Notion â†’ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ea3863c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7351,
     "status": "ok",
     "timestamp": 1757696983858,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "ea3863c5",
    "outputId": "10d8f618-5604-4014-c107-f93d46477d75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " [('ìŠ¤í„°ë””ë°©í–¥ì„±', 'https://www.notion.so/24977d75e00880369b93e2653613eec3'),\n",
       "  ('ê³„íš', 'https://www.notion.so/22e77d75e00880ea9085d1410be465fb')])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for dbid in DATABASE_IDS:\n",
    "    docs += fetch_pages_from_database(dbid)\n",
    "\n",
    "for pid in PAGE_IDS:\n",
    "    docs.append(fetch_single_page(pid))\n",
    "\n",
    "len(docs), [ (d['title'], d['url']) for d in docs[:5] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ZxNlgm0o3Bx4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1757696986642,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "ZxNlgm0o3Bx4",
    "outputId": "4ac8eff2-d2a1-453c-a2a7-9103a8369e70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_id': '24977d75-e008-8036-9b93-e2653613eec3',\n",
       "  'title': 'ìŠ¤í„°ë””ë°©í–¥ì„±',\n",
       "  'url': 'https://www.notion.so/24977d75e00880369b93e2653613eec3',\n",
       "  'last_edited_time': '2025-08-24T04:13:00.000Z',\n",
       "  'content_md': 'ì¹´í…Œê³ ë¦¬ | ë§ì´ ì“°ëŠ” ë„êµ¬ | ì´ìœ  / í˜„ì—… ì‚¬ìš© í¬ì¸íŠ¸\\në°ì´í„° ë²„ì „ê´€ë¦¬ | DVC, Git LFS | DVCëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì¶”ì  + ì›ê²© ì €ì¥ ì—°ë™ì— ê°•í•¨, Git LFSëŠ” ë‹¨ìˆœ ëŒ€ìš©ëŸ‰ íŒŒì¼ ë²„ì „ ê´€ë¦¬\\níŒŒì´í”„ë¼ì¸ ê´€ë¦¬ | Airflow, Prefect | AirflowëŠ” ëŒ€ê·œëª¨ ê¸°ì—…Â·ë°°ì¹˜ ì‘ì—… í‘œì¤€, PrefectëŠ” íŒŒì´ì¬ ì¹œí™”ì ì´ê³  ì„¤ì •ì´ ë‹¨ìˆœ\\nì‹¤í—˜ ê´€ë¦¬ | MLflow, wandb | MLflowëŠ” ì˜¤í”ˆì†ŒìŠ¤ í‘œì¤€, wandbëŠ” ì‹œê°í™”Â·í˜‘ì—… ê¸°ëŠ¥ì´ ê°•í•˜ê³  ìŠ¤íƒ€íŠ¸ì—…/ì—°êµ¬ì‹¤ì—ì„œ ì¸ê¸°\\në¶„ì‚° í•™ìŠµ | PyTorch DDP, Ray | ëŒ€ê·œëª¨ í•™ìŠµì€ DDPê°€ í‘œì¤€, RayëŠ” DDP ë˜í•‘ + Task ë¶„ì‚° ë‘˜ ë‹¤ ê°€ëŠ¥\\nëª¨ë¸ ì„œë¹™ | FastAPI, BentoML | FastAPIëŠ” ê°€ë³ê³  ìœ ì—°, BentoMLì€ ëª¨ë¸ ë²„ì „ ê´€ë¦¬ + ì„œë¹™ ì¼ì²´í˜•\\në°°í¬ | Docker, Kubernetes(K8s) | DockerëŠ” ê°œë°œÂ·í…ŒìŠ¤íŠ¸ í•„ìˆ˜, ìš´ì˜ í™˜ê²½ í™•ì¥ì€ K8sê°€ ì‚¬ì‹¤ìƒ í‘œì¤€\\nëª¨ë‹ˆí„°ë§ | Prometheus + Grafana, Evidently AI | Prom+GrafëŠ” ì‹œìŠ¤í…œ/ëª¨ë¸ ì§€í‘œ ëª¨ë‹ˆí„°ë§ í‘œì¤€, EvidentlyëŠ” ë°ì´í„° ë“œë¦¬í”„íŠ¸Â·ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ íŠ¹í™”\\n- DHKimì—­ëŸ‰: ì‹¤í—˜ ê´€ë¦¬, ë¶„ì‚°í•™ìŠµ, ë°°í¬, ëª¨ë‹ˆí„°ë§ ìª½ ê°€ëŠ¥ \\n- ì‹¤í—˜ê´€ë¦¬ (visualization): MLflow, wandb\\n- ë¶„ì‚°í•™ìŠµ: Ray, PyTorchDDP, Accelerate ë“±\\n- ë°°í¬: Docker\\n- ëª¨ë‹ˆí„°ë§: Prometheus + Grafana (í˜„ì¬ ì—°êµ¬ì‹¤ GPUëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë§Œë“ ì´ë ¥ìˆìŒ)\\n- promtQL\\nìƒˆë¡œë°°ì›Œì•¼ë  í•­ëª©\\n- DVC, Airflow, Prefect, Evidently AI\\nì˜ì—­ | í•  ìˆ˜ ìˆëŠ” ê²ƒ | ëŒ€í‘œ ì»´í¬ë„ŒíŠ¸/í‚¤ì›Œë“œ | ì˜ˆì‹œ ì‚¬ìš©ì²˜\\ní”„ë¡¬í”„íŠ¸ êµ¬ì„± | í…œí”Œë¦¿Â·ë³€ìˆ˜ ë°”ì¸ë”©Â·ì²´ì´ë‹ | PromptTemplate, ChatPromptTemplate | ì—­í• ì§€ì‹œ + ì‚¬ìš©ì ì…ë ¥ ì¡°í•©í•´ì„œ ì¼ê´€ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±\\nëª¨ë¸ í˜¸ì¶œ ë˜í•‘ | ë‹¤ì–‘í•œ LLM/ì±—ëª¨ë¸/ì„ë² ë”© í˜¸ì¶œ | ChatOpenAI, ChatAnthropic, HuggingFaceEndpoints, OpenAIEmbeddings | OpenAI/Anthropic/ë¡œì»¬ LLM ë“± í†µí•© ì¸í„°í˜ì´ìŠ¤\\nêµ¬ì¡°í™”ëœ ì¶œë ¥ | JSON/ìŠ¤í‚¤ë§ˆ ê°•ì œ, íŒŒì„œ | PydanticOutputParser, StructuredOutputParser, í•¨ìˆ˜/íˆ´ í˜¸ì¶œ | API ì‘ë‹µì²˜ëŸ¼ JSON ìŠ¤í‚¤ë§ˆ ë³´ì¥\\nì²´ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ | ë‹¨ê³„ ì—°ê²°, ì¡°ê±´ ë¶„ê¸°, ë³‘ë ¬/ìŠ¤íŠ¸ë¦¬ë° | LCEL(Runnable*), map, batch, stream | â€œì§ˆì˜â†’ê²€ìƒ‰â†’ìš”ì•½â€ íŒŒì´í”„ë¼ì¸ í•œ ì¤„ë¡œ êµ¬ì„±\\në©”ëª¨ë¦¬ | ëŒ€í™” íˆìŠ¤í† ë¦¬/ìš”ì•½/í‚¤ê°’ ë©”ëª¨ë¦¬ | ConversationBufferMemory, BufferWindow, SummaryMemory | ë©€í‹°í„´ ì±—ë´‡ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\\në¬¸ì„œ ë¡œë”© | ì›¹/íŒŒì¼/ë…¸ì…˜/êµ¬ê¸€ë“œë¼ì´ë¸Œ ë“± ë¡œë” | DocumentLoaders (PDF/HTML/Notion/GDrive/S3/Playwright ë“±) | ì‚¬ë‚´Â·ì™¸ë¶€ ë¬¸ì„œ ì½ì–´ì˜¤ê¸°\\ní…ìŠ¤íŠ¸ ë¶„í•  | ê¸¸ì´ ê¸°ë°˜/ì˜ë¯¸ ê¸°ë°˜ ìŠ¤í”Œë¦¿ | RecursiveCharacterTextSplitter, Markdown/Text splitters | ê¸´ ë¬¸ì„œë¥¼ RAG ì¹œí™”ì ìœ¼ë¡œ ìª¼ê°œê¸°\\nì„ë² ë”© & ë²¡í„°DB | ì„ë² ë”© ìƒì„±Â·ì €ì¥Â·ê²€ìƒ‰ | FAISS, Chroma, Pinecone, Weaviate, Milvus | RAG ì¸ë±ìŠ¤ êµ¬ì¶•\\në¦¬íŠ¸ë¦¬ë²„ (RAG) | ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ | VectorStoreRetriever, MultiQueryRetriever, ContextualCompressionRetriever, SelfQueryRetriever | ë©€í‹°ì¿¼ë¦¬Â·í•„í„°Â·ìš”ì•½ ì••ì¶• ê²€ìƒ‰\\nRerank/ì••ì¶• | ì»¨í…ìŠ¤íŠ¸ ì¬ë­í‚¹Â·ìš”ì•½ ì••ì¶• | ContextualCompression, LLMChainExtractor, (BM25 ë“± í†µí•©) | ë…¸ì´ì¦ˆ ì¤„ì´ê³  ì •ë‹µ ê·¼ê±°ë§Œ ì „ë‹¬\\nì¿¼ë¦¬ ë³€í™˜ | ì§ˆì˜ ì¬ì‘ì„±/í™•ì¥/í•˜ì´í”ˆ | QueryTransformation, HypotheticalDocumentEmbedding(HyDE) | ëª¨æ¨¡í˜¸ê°€í•œ ì§ˆë¬¸ì„ ê²€ìƒ‰ ì¹œí™”ì ìœ¼ë¡œ ë³€ê²½\\nì¸ë±ìŠ¤/ê·¸ë˜í”„ | SQL/ê·¸ë˜í”„/íŒŒì¼ í˜¼í•© ì§ˆì˜ | SQLDatabaseChain, Cypher QA(Neo4j) | â€œìì—°ì–´â†’SQL/ì‚¬ì´í¼â€ ì§ˆì˜\\nì—ì´ì „íŠ¸ | íˆ´ ì‚¬ìš©Â·ê³„íš ì‹¤í–‰ | AgentExecutor, Tool, ReAct, í•¨ìˆ˜/íˆ´ í˜¸ì¶œ | ì›¹ê²€ìƒ‰â†’ì½”ë“œì‹¤í–‰â†’ìš”ì•½ ê°™ì€ ë©€í‹°ìŠ¤í… ìë™í™”\\níˆ´ ì—°ë™ | ì½”ë“œ/ê²€ìƒ‰/DB/ì›¹ ë“± | PythonREPLTool, RequestsTool/Browser, SQLTool, ShellTool | ì½”ë“œ ìŠ¤ë‹ˆí« ì‹¤í–‰, REST í˜¸ì¶œ, DB ì§ˆì˜\\ní‰ê°€/Eval | ìë™ í‰ê°€Â·ì±„ì Â·ë§¨ì…˜ | langchain.evaluation(QA/ë§ì¶¤ í‰ê°€ì) | RAG ì‘ë‹µ ì •í™•ë„/ê·¼ê±° ì¼ì¹˜ì„± ì¸¡ì •\\nì½œë°±/ë¡œê¹… | í† í°/ì§€ì—°ì‹œê°„/ì¤‘ê°„ë‹¨ê³„ ë¡œê¹… | Callbacks, (LangSmith ì—°ë™) | ë””ë²„ê¹…Â·íŠ¸ë ˆì´ì‹±\\nìºì‹œ | ëª¨ë¸ ì‘ë‹µ ìºì‹± | InMemoryCache, Redis/SQLite Cache | ë¹„ìš© ì ˆê°Â·ì†ë„ í–¥ìƒ\\ní¼ì‹œìŠ¤í„´ìŠ¤ | ì²´í¬í¬ì¸íŠ¸/ìƒíƒœ ì €ì¥(ê²½ëŸ‰) | Checkpoint, History | ê¸´ ì›Œí¬í”Œë¡œ ì¬ì‹œì‘\\ní”„ë¡¬í”„íŠ¸ ê³µìœ  | í”„ë¡¬í”„íŠ¸Â·ì²´ì¸ ë ˆì‹œí”¼ ê³µìœ  | LangChain Hub | íŒ€ ë‚´ í‘œì¤€ í…œí”Œë¦¿ ì¬ì‚¬ìš©\\n- ì˜ ëª¨ë¥´ì§€ë§Œ ì´ëŸ°ê²ƒë“¤ì„ í•  ìˆ˜ ìˆìŒ\\n- ì¸ì›: MLOps 2ëª… / LangChain 2ëª…\\n- ì§„í–‰: ì£¼ 1íšŒ(2ì‹œê°„) \\n- ê° ë¶„ì•¼ 1ì‹œê°„ì”©: ê°œë… ì„¤ëª…(20ë¶„) + ì‹¤ìŠµ ë°ëª¨(30ë¶„) + Q&A(10ë¶„)\\n- ìˆ˜ì—…ìë£Œ: Jupyter Notebook + ìŠ¬ë¼ì´ë“œ or ë¸”ë¡œê·¸ ìš”ì•½(ì„ íƒ) â†’ GitHub ì—…ë¡œë“œ\\n- ê¸°ê°„: 4~6ê°œì›”\\nëª©í‘œ: ê°œë… ì¡ê¸° + í™˜ê²½ ì…‹ì—… + ê°„ë‹¨í•œ ì‹¤ìŠµ\\nì£¼ì°¨ | MLOpsíŒ€ | LangChainíŒ€\\n1ì£¼ | MLOps ê°œìš”, êµ¬ì„±ìš”ì†Œ | LangChain ê°œìš”, êµ¬ì„±ìš”ì†Œ\\n2ì£¼ | Docker ê¸°ì´ˆ | LLMChain, PromptTemplate\\n3ì£¼ | DVCë¡œ ë°ì´í„° ë²„ì „ ê´€ë¦¬ | DocumentLoader, TextSplitter\\n4ì£¼ | MLflow ì‹¤í—˜ ê¸°ë¡ | Embedding + VectorStore\\n5ì£¼ | ë³µìŠµ/ë³´ì¶© ì£¼ì°¨ | ë³µìŠµ/ë³´ì¶© ì£¼ì°¨\\n6ì£¼ | Prefect or Airflow íŒŒì´í”„ë¼ì¸ | Retriever & RAG ê¸°ë³¸\\nëª©í‘œ: ê°œë³„ ë¶„ì•¼ ì‹¬í™”, ì‹¤ì „ ê¸°ëŠ¥ êµ¬í˜„\\nì£¼ì°¨ | MLOpsíŒ€ | LangChainíŒ€\\n7ì£¼ | Ray ë¶„ì‚° ì²˜ë¦¬ ê¸°ì´ˆ | MultiQueryRetriever, Context ì••ì¶•\\n8ì£¼ | PyTorch DDP | Tool & Agent ê¸°ì´ˆ\\n9ì£¼ | FastAPI ëª¨ë¸ ì„œë¹™ | ì™¸ë¶€ API ì—°ë™ Agent\\n10ì£¼ | Docker-compose í™˜ê²½ êµ¬ì„± | RAG+Agent ê²°í•©\\n11ì£¼ | ë³µìŠµ/ë³´ì¶© ì£¼ì°¨ | ë³µìŠµ/ë³´ì¶© ì£¼ì°¨\\n12ì£¼ | Kubernetes ê¸°ì´ˆ | LangChain ë©”ëª¨ë¦¬/ìºì‹œ\\nëª©í‘œ: ì–‘ íŒ€ ì—°ê²°, ë°°í¬ ê°€ëŠ¥ ìƒíƒœê¹Œì§€\\nì£¼ì°¨ | MLOpsíŒ€ | LangChainíŒ€\\n13ì£¼ | MLflow ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ | ë°ì´í„° ì „ì²˜ë¦¬ ìë™í™”\\n14ì£¼ | CI/CD (GitHub Actions) | LangChain í‰ê°€(evaluation)\\n15ì£¼ | ëª¨ë‹ˆí„°ë§(Prom+Graf or Evidently) | API â†’ UI ì—°ë™(Streamlit)\\n16ì£¼ | í†µí•© ë¦¬í—ˆì„¤ | í†µí•© ë¦¬í—ˆì„¤\\n17~18ì£¼* | ë²„í¼ ì£¼ì°¨ (ì˜ˆì •ì¹˜ ëª»í•œ ì§€ì—° ëŒ€ë¹„) | ë²„í¼ ì£¼ì°¨\\n- ì‹¤ìŠµ ìµœì†Œ ë‹¨ìœ„: Jupyterì—ì„œ 30~50ì¤„ ë‚´ì™¸ ì½”ë“œë¡œ ëë‚¼ ìˆ˜ ìˆëŠ” ì˜ˆì œë¡œ ì œí•œ.\\n- ë³µìŠµ ì£¼ì°¨ í•„ìˆ˜: 4~5ì£¼ë§ˆë‹¤ í•œ ë²ˆì€ ë°œí‘œ ì—†ì´ ì§€ë‚œ ë‚´ìš© ì‹¤ìŠµ/ì½”ë“œ ìˆ˜ì •ë§Œ.\\n- í‰ì¼: í•˜ë£¨ 20~30ë¶„, ë°œí‘œ ì¤€ë¹„ë‚˜ ì½”ë“œ ì‘ì„±\\n- ì£¼ë§: ëª¨ì„(2ì‹œê°„) + ë°œí‘œ/ì§ˆë¬¸\\n- ë§¤ì£¼: ë°œí‘œ ëë‚˜ë©´ ë°”ë¡œ GitHub ì—…ë¡œë“œ (ì •ë¦¬ ìµœì†Œí™”, ì½”ë“œ+ê°„ë‹¨ì„¤ëª…)\\n- ëë‚˜ê³ , ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¡œ Configuration ìƒì„±ê¸° Agent ë¥¼ ë§Œë“¤ì–´ë³´ëŠ”ê±´ ì–´ë–¨ê¹Œ?\\nMLOpsëŠ” ë³´í†µ config.jsonì´ êµ‰ì¥íˆ ê¸¸ì–´ì§ (Modelê´€ë ¨, metricê´€ë ¨, datasetê´€ë ¨, ê°ê° í•¨ìˆ˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ ë‹¤ ì…ë ¥í•´ì•¼í•˜ë‹ˆê¹Œ) ì´ˆì°½ê¸° ëŸ¬ë‹ì»¤ë¸Œê°€ ì‹¬í•´ì§\\nâ‡’ ë”°ë¼ì„œ, LangChainê¸°ë°˜ Agentë¥¼ ìš°ë¦¬ê°€ êµ¬ì¶•í•œ ê°„ë‹¨í•œ MLOps íŒŒì´í”„ë¼ì¸ì˜ config íŒŒì¼ì„ ìœ ì €ì˜ queryì— ë”°ë¼ ë§ì¶¤í˜•ìœ¼ë¡œ json ì´ë‚˜ yamlíŒŒì¼ì„ ìƒì„±í•´ì£¼ëŠ” chatbotì„ ë§Œë“œëŠ” ê²ƒë„ ì¢‹ì•„ë³´ì„\\nì˜ˆë¥¼ë“¤ë©´,\\n> ìœ ì €:\\n\"mnist ë°ì´í„°ë¡œ ResNet+UNet ì„ë² ë”©, ViT+MLP ë¶„ë¥˜ë¥¼ ê°ê° ëŒë ¤ë³´ê³  ì‹¶ë‹¤.\"\\n> Agent ì‘ë‹µ:\\n\"ìš”ì²­í•˜ì‹  êµ¬ì¡°ì— ë§ì¶° ResNet+UNet configì™€ ViT+MLP configë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\\nê²½ë¡œ: ./config/resnet_unet.yaml, ./config/vit_mlp.yaml\\nê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì„¸íŒ…í–ˆìœ¼ë©°, GPU 1ê°œ ê¸°ì¤€ì…ë‹ˆë‹¤.\"\\nMLOps: ê°„ë‹¨í•œ ìë™í™” íŒŒì´í”„ë¼ì¸\\nLangChain: ìœ ì € ì¿¼ë¦¬ ì²˜ë¦¬ (ê¸°ì¡´ configíŒŒì¼ ì°¸ê³ :Rag) ì‘ë‹µìƒì„± â†’ í…œí”Œë¦¿ê¸°ë°˜ íŒŒì¼ ìƒì„± â†’ íŒŒì¼ ì €ì¥ ìˆ˜í–‰ (ê¸°ëŠ¥ìˆ˜í–‰) ì´ëŸ° íë¦„ìœ¼ë¡œ ë„êµ¬í™”ê°€ ê°€ëŠ¥í•´ë³´ì„\\n- ë”í•˜ë©´ ì•„ì˜ˆ MLOps ìë™í™” Agentì¼í…ë°.. ì´ê±´ë„ˆë¬´ ë¹¡ì„¸ë³´ì„'},\n",
       " {'page_id': '22e77d75-e008-80ea-9085-d1410be465fb',\n",
       "  'title': 'ê³„íš',\n",
       "  'url': 'https://www.notion.so/22e77d75e00880ea9085d1410be465fb',\n",
       "  'last_edited_time': '2025-08-24T04:06:00.000Z',\n",
       "  'content_md': 'Week 1~8\\nWeek 1 â€“ ì •ë ¬ & ë¦¬ìŠ¤íŠ¸ í™œìš©\\ní•µì‹¬ ê°œë…: sorted(), sort(), lambda, ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì¡°ì‘\\n- Kë²ˆì§¸ ìˆ˜ (Lv.1)\\n- ë¬¸ìì—´ ë‚´ ë§ˆìŒëŒ€ë¡œ ì •ë ¬í•˜ê¸° (Lv.1) \\n- ìµœëŒ“ê°’ê³¼ ìµœì†Ÿê°’ (Lv.2)\\n- ìµœëŒ“ê°’ ë§Œë“¤ê¸° (1) (Lv.0)\\n- ë¬¸ìì—´ ì •ë ¬í•˜ê¸° (1) (Lv.0)\\nWeek 2 â€“ ì™„ì „íƒìƒ‰ ê¸°ë³¸\\ní•µì‹¬ ê°œë…: forë¬¸ì„ í™œìš©í•œ ì „ì²´ íƒìƒ‰, ì¤‘ì²© ë°˜ë³µë¬¸ êµ¬ì¡°\\n- ëª¨ì˜ê³ ì‚¬ (Lv.1)\\n- ì†Œìˆ˜ ì°¾ê¸° (Lv.1)\\n- ì‚¼ì´ì‚¬ (Lv.1)\\n- í•œ ë²ˆë§Œ ë“±ì¥í•œ ë¬¸ì (Lv.0)\\n- ì†Œìˆ˜ ë§Œë“¤ê¸° (Lv.1)\\nWeek 3 â€“ ì¡°ê±´ + êµ¬í˜„ + ì‹œë®¬ë ˆì´ì…˜\\ní•µì‹¬ ê°œë…: ì¡°ê±´ ë¶„ê¸°ë¬¸, ê°’ ëˆ„ì , ë°˜ë³µë¬¸ íë¦„ ì„¤ê³„\\n- ë¡œë˜ì˜ ìµœê³  ìˆœìœ„ì™€ ìµœì € ìˆœìœ„ (Lv.1)\\n- ì‹ ê³  ê²°ê³¼ ë°›ê¸° (Lv.1)\\n- ëª…ì˜ˆì˜ ì „ë‹¹ (1) (Lv.1)\\n- ê°œì¸ì •ë³´ ìˆ˜ì§‘ ìœ íš¨ê¸°ê°„ (Lv.1~2)\\n- í–„ë²„ê±° ë§Œë“¤ê¸° (Lv.1)\\nWeek 4: í•´ì‹œ (Hash Table)\\ní•µì‹¬ ê°œë…: ë”•ì…”ë„ˆë¦¬ë¥¼ í™œìš©í•œ ë¹ ë¥¸ íƒìƒ‰, Key-Value êµ¬ì¡°, dict, .get(), .items(), .setdefault()\\n- ì™„ì£¼í•˜ì§€ ëª»í•œ ì„ ìˆ˜ (Lv.1)\\n- ì˜ìƒ (Lv.2)\\n- í°ì¼“ëª¬ (Lv.1)\\n- ë² ìŠ¤íŠ¸ì•¨ë²” (Lv.3)\\n- ì„±ê²© ìœ í˜• ê²€ì‚¬í•˜ê¸° (Lv.1)\\nWeek 5: ê·¸ë¦¬ë”” (Greedy) \\ní•µì‹¬ ê°œë…: í˜„ì¬ì˜ ìµœì í•´ê°€ ì „ì²´ ë¬¸ì œì˜ ìµœì í•´ê°€ ë˜ëŠ” ê²½ìš° íƒìƒ‰\\n- ì²´ìœ¡ë³µ (Lv.1)\\n- í° ìˆ˜ ë§Œë“¤ê¸° (Lv.2)\\n- ì¡°ì´ìŠ¤í‹± (Lv.2, ì‹¬í™”)\\n- êµ¬ëª…ë³´íŠ¸ (Lv.2)\\n- ATM (ë°±ì¤€, S4) \\nWeek 6: ìŠ¤íƒ/í\\ní•µì‹¬ ê°œë…: ì„ ì…ì„ ì¶œ(FIFO), í›„ì…ì„ ì¶œ(LIFO) ìë£Œêµ¬ì¡° í™œìš©, collections.deque, stack.append(), stack.pop()\\n- ê°™ì€ ìˆ«ìëŠ” ì‹«ì–´ (Lv.1)\\n- ì˜¬ë°”ë¥¸ ê´„í˜¸ (Lv.2)\\n- ê¸°ëŠ¥ê°œë°œ (Lv.2)\\n- í”„ë¦°í„° (Lv.2) í”„ë¡œì„¸ìŠ¤ (Lv.2)\\n- ê´„í˜¸ íšŒì „í•˜ê¸° (Lv.2)\\nWeek 7: ì´ë¶„íƒìƒ‰ (Binary Search)\\ní•µì‹¬ ê°œë…: ì •ë ¬ëœ ë°°ì—´ì—ì„œ íš¨ìœ¨ì ì¸ ê°’ íƒìƒ‰\\n- ì •ìˆ˜ ì œê³±ê·¼ íŒë³„ (Lv.1)\\n- ì˜ˆì‚° (Lv.1)\\n- ë‚˜ë¬´ ìë¥´ê¸° (ë°±ì¤€, S2)\\n- ì…êµ­ì‹¬ì‚¬ (Lv.3)\\n- ìˆ«ì ì¹´ë“œ2 (ë°±ì¤€, S4)\\nWeek 8: DFS / BFS (ê·¸ë˜í”„ íƒìƒ‰ ê¸°ë³¸)\\ní•µì‹¬ ê°œë…: ë°©ë¬¸ ì²´í¬ë¥¼ í†µí•œ ê·¸ë˜í”„ íƒìƒ‰, visited[], ì¬ê·€ í•¨ìˆ˜, deque í™œìš©\\n- íƒ€ê²Ÿ ë„˜ë²„ (DFS, Lv.2)\\n- ë„¤íŠ¸ì›Œí¬ (DFS/BFS, Lv.2)\\n- ë‹¨ì–´ ë³€í™˜ (Lv.3)\\n- ê²Œì„ ë§µ ìµœë‹¨ê±°ë¦¬ (BFS, Lv.2)\\n- ë¯¸ë¡œ íƒìƒ‰ (ë°±ì¤€, S1)\\nWeek 9 â€“ ë™ì  ê³„íšë²• (DP) [ë™í¬]\\ní•µì‹¬ ê°œë…: ì í™”ì‹ ì •ì˜, ë©”ëª¨ì´ì œì´ì…˜ (dp[]) 1ì°¨ì› vs 2ì°¨ì› DP êµ¬ì¡° ì´í•´\\n- í”¼ë³´ë‚˜ì¹˜ ìˆ˜ (Lv.2)\\n- ë©€ë¦¬ ë›°ê¸° (Lv.2)\\n- Nìœ¼ë¡œ í‘œí˜„ (Lv.3)\\n- ì •ìˆ˜ ì‚¼ê°í˜• (Lv.3)\\n- ë“±êµ£ê¸¸ (Lv.3)\\n- ì‚¬ì¹™ì—°ì‚° (Lv.4)\\n- ë„ë‘‘ì§ˆ (Lv.4)\\nWeek 10 â€“ ìµœë‹¨ ê²½ë¡œ & ë‹¤ìµìŠ¤íŠ¸ë¼ (Dijkstra) [ì˜ˆë¦°]\\ní•µì‹¬ ê°œë…: ê°€ì¤‘ì¹˜ ìˆëŠ” ê·¸ë˜í”„ì—ì„œ ìµœë‹¨ ê²½ë¡œ íƒìƒ‰, distance[], heapq, ìš°ì„ ìˆœìœ„ í\\n- ë°°ë‹¬ (Lv.2)\\n- ë“±êµ£ê¸¸ (Lv.3)\\n- ê°€ì¥ ë¨¼ ë…¸ë“œ (Lv.3) \\n- í•©ìŠ¹ íƒì‹œ ìš”ê¸ˆ (Lv.3)\\n- ë¶€ëŒ€ë³µê·€ (Lv.3)\\n- ë””ìŠ¤í¬ ì»¨íŠ¸ë¡¤ëŸ¬ (Lv.3) \\n- ìˆ¨ë°”ê¼­ì§ˆ 3 (ë°±ì¤€ 13549)\\nWeek 11 â€“ ë°±íŠ¸ë˜í‚¹ & ìœ ë‹ˆì˜¨ íŒŒì¸ë“œ[ìˆ˜ë¹ˆ]\\ní•µì‹¬ ê°œë…: ë°±íŠ¸ë˜í‚¹: ì™„ì „íƒìƒ‰ + ì¡°ê±´ ë¶„ê¸° ìµœì í™”, ìœ ë‹ˆì˜¨ íŒŒì¸ë“œ (find, union)\\n- N-Queen (Lv.3)\\n- ì—¬í–‰ ê²½ë¡œ (Lv.3)\\n- ìˆ˜ì‹ ìµœëŒ€í™” (Lv.2)\\n- ì„¬ ì—°ê²°í•˜ê¸° (Lv.3) \\nWeek 12 â€“  ëª¨ì˜ê³ ì‚¬\\n- ê´€ì‹¬ ë¶„ì•¼ ë­”ì§€ ì´ë¦„ ë„£ê¸°~ ì¤‘ë³µ ìƒê´€ì—†ìŒ~\\n- MLOps\\n- Ray (pytorch-multi processing ë° HPO ìë™í™”)\\n- LangChain\\n- RAG, MCP\\n- LLMê¸°ë°˜ AgentAI, Multi-Agent(ìˆ©)\\n- Foundation Model Tuning\\n- Fine tuning\\n- LoRA\\n- Transfer or Knowledge Distillation\\n- Multi-modal(ìˆ©)\\n- \\n- 8ì›” 9ì¼ì€ ëª¨ì˜ê³ ì‚¬ í‘¸ëŠ”ë‚ ~'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39bfac",
   "metadata": {
    "id": "9e39bfac"
   },
   "source": [
    "## 4) ì „ì²˜ë¦¬ & ì²­í‚¹(Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4921e2cc",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1757696989472,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "4921e2cc"
   },
   "outputs": [],
   "source": [
    "def split_markdown(md: str, max_len=900):\n",
    "    parts=[]; buf=[]\n",
    "    for line in md.splitlines():\n",
    "        if re.match(r\"^#{1,6}\\s\", line) and buf:\n",
    "            chunk=\"\\n\".join(buf).strip()\n",
    "            parts += textwrap.wrap(chunk, max_len, break_long_words=False, break_on_hyphens=False) if len(chunk)>max_len else [chunk]\n",
    "            buf=[line]\n",
    "        else:\n",
    "            buf.append(line)\n",
    "    if buf:\n",
    "        chunk=\"\\n\".join(buf).strip()\n",
    "        parts += textwrap.wrap(chunk, max_len, break_long_words=False, break_on_hyphens=False) if len(chunk)>max_len else [chunk]\n",
    "    return [p for p in parts if p.strip()]\n",
    "\n",
    "chunks=[]\n",
    "metas=[]\n",
    "for d in docs:\n",
    "    for ch in split_markdown(d[\"content_md\"]):\n",
    "        metas.append({\"page_id\": d[\"page_id\"], \"title\": d[\"title\"], \"url\": d.get(\"url\"), \"section\": \"\", \"text\": ch})\n",
    "        chunks.append(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62ceb2",
   "metadata": {
    "id": "8b62ceb2"
   },
   "source": [
    "## 5) ì„ë² ë”© & ë²¡í„° ì¸ë±ìŠ¤(FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ad104804",
   "metadata": {
    "executionInfo": {
     "elapsed": 66293,
     "status": "ok",
     "timestamp": 1757697056971,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "ad104804"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "e_model = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "def embed(texts):\n",
    "    return e_model.encode(texts, normalize_embeddings=True, convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "vecs = embed(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dbb5e9a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1757697056997,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "dbb5e9a4",
    "outputId": "c05c4edc-761c-43ee-98e2-5483cd7fa350"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, faiss\n",
    "\n",
    "class FaissStore:\n",
    "    def __init__(self, dim):\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.meta = []\n",
    "    def add(self, vecs, metas):\n",
    "        self.index.add(vecs)    # í•™ìŠµ ë¶ˆí•„ìš”, ë°”ë¡œ ì¶”ê°€\n",
    "        self.meta += metas\n",
    "    def search(self, qvec, k=5):\n",
    "        D,I = self.index.search(np.array([qvec]).astype(\"float32\"), k)  # ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ kê°œ\n",
    "        out=[]\n",
    "        for rank, idx in enumerate(I[0]):\n",
    "            if idx == -1: continue\n",
    "            m = self.meta[idx]\n",
    "            out.append({\"text\": m[\"text\"], \"meta\": {k:v for k,v in m.items() if k!=\"text\"}, \"score\": float(D[0][rank])})\n",
    "        return out\n",
    "\n",
    "store = FaissStore(vecs.shape[1])\n",
    "store.add(vecs, metas)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130bcdd2",
   "metadata": {
    "id": "130bcdd2"
   },
   "source": [
    "## 6) LLM í˜¸ì¶œ (OpenAI-í˜¸í™˜) ë° ì§ˆì˜ â†’ ê²€ìƒ‰ â†’ ë‹µë³€\n",
    "- ê¸°ë³¸ OpenRouter, í•„ìš”ì‹œ BASE_URL/MODEL_NAME êµì²´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "75e81177",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2588,
     "status": "ok",
     "timestamp": 1757700822498,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "75e81177",
    "outputId": "cc575d73-bfd5-4948-d523-d522a6a0f8c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ë‹µë³€]\n",
      " Week 3 ê³„íšì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "- ë¡œë˜ì˜ ìµœê³  ìˆœìœ„ì™€ ìµœì € ìˆœìœ„ (Lv.1)\n",
      "- ì‹ ê³  ê²°ê³¼ ë°›ê¸° (Lv.1)\n",
      "- ëª…ì˜ˆì˜ ì „ë‹¹ (1) (Lv.1)\n",
      "- ê°œì¸ì •ë³´ ìˆ˜ì§‘ ìœ íš¨ê¸°ê°„ (Lv.1~2)\n",
      "- í–„ë²„ê±° ë§Œë“¤ê¸° (Lv.1)\n",
      "\n",
      "ì´ ê³„íšì€ ì¡°ê±´ + êµ¬í˜„ + ì‹œë®¬ë ˆì´ì…˜ í•µì‹¬ ê°œë…ì„ í•™ìŠµí•˜ëŠ” ì£¼ì°¨ì…ë‹ˆë‹¤.\n",
      "\n",
      "[ê·¼ê±°]\n",
      "(1) ê³„íš | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(2) ê³„íš | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(3) ê³„íš | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(4) ìŠ¤í„°ë””ë°©í–¥ì„± | https://www.notion.so/24977d75e00880369b93e2653613eec3\n",
      "(5) ìŠ¤í„°ë””ë°©í–¥ì„± | https://www.notion.so/24977d75e00880369b93e2653613eec3\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"PROVIDER_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "SYSTEM = \"ë‹¹ì‹ ì€ ì‹ ë¢° ê°€ëŠ¥í•œ í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ê·¼ê±° ì™¸ ì¶”ì¸¡ ê¸ˆì§€.\"\n",
    "\n",
    "def build_prompt(query, contexts):\n",
    "    ctx = \"\\n\\n---\\n\\n\".join(\n",
    "        f\"[{i+1}] {c['meta'].get('title','(ì œëª©ì—†ìŒ)')} / {c['meta'].get('section','')}\\n{c['text']}\"\n",
    "        for i,c in enumerate(contexts)\n",
    "    )\n",
    "    return f\"\"\"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n",
    "\n",
    "ë‹¤ìŒ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì •í™•íˆ ë‹µí•˜ì„¸ìš”.\n",
    "ê·¼ê±°:\n",
    "{ctx}\n",
    "\n",
    "ê·œì¹™:\n",
    "- ê·¼ê±°ì— ì—†ëŠ” ë‚´ìš©ì€ 'ê·¼ê±° ì—†ìŒ'ìœ¼ë¡œ í‘œì‹œ\n",
    "- í•„ìš”í•œ ê²½ìš° ëª©ë¡/í‘œë¡œ ê°„ê²°íˆ\n",
    "- ê° ì£¼ì¥ì—ëŠ” ê·¼ê±° ë²ˆí˜¸ë¥¼ ë¶™ì—¬ë¼\n",
    "\"\"\"\n",
    "\n",
    "def llm_answer(query, contexts, temperature=0.2, max_tokens=800):\n",
    "    prompt = build_prompt(query, contexts)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\":\"system\",\"content\":SYSTEM}, {\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "\n",
    "def embed_one(text):\n",
    "    return embed([text])[0]\n",
    "\n",
    "def ask(q: str, k: int = 8, n_ctx: int = 5):\n",
    "    qv = embed_one(q)\n",
    "    cands = store.search(qv, k=k)\n",
    "    contexts = cands[:n_ctx]\n",
    "    answer = llm_answer(q, contexts)\n",
    "    print(\"\\n[ë‹µë³€]\\n\", answer)\n",
    "    print(\"\\n[ê·¼ê±°]\")\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        print(f\"({i}) {c['meta']['title']} | {c['meta'].get('url','')}\")\n",
    "    return answer, contexts\n",
    "\n",
    "# ì˜ˆì‹œ ì‹¤í–‰:\n",
    "answer, ctx = ask(\"Week 3 ê³„íš\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba047b8",
   "metadata": {
    "id": "aba047b8"
   },
   "source": [
    "\n",
    "---\n",
    "# Aì•ˆ: 6ë‹¨ê³„ë§Œ LangChainìœ¼ë¡œ ê°ì‹¸ê¸° (ìµœì†Œí•œë§Œ LangChainìœ¼ë¡œ ë³€í™˜)\n",
    "\n",
    "- 0)~5)ê¹Œì§€ëŠ” **ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©**\n",
    "- 6) **í”„ë¡¬í”„íŠ¸â†’LLM** ë¶€ë¶„ë§Œ LangChain ì²´ì¸ìœ¼ë¡œ ì„ ì–¸ì ìœ¼ë¡œ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0670aec8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4239,
     "status": "ok",
     "timestamp": 1757700955525,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "0670aec8",
    "outputId": "0e3763cc-68da-42e6-f2ba-0023faae264a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ë‹µë³€]\n",
      " Week 3 ê³„íšì— ëŒ€í•œ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "- Week 3 â€“ ì¡°ê±´ + êµ¬í˜„ + ì‹œë®¬ë ˆì´ì…˜ í•µì‹¬ ê°œë…: ì¡°ê±´ ë¶„ê¸°ë¬¸, ê°’ ëˆ„ì , ë°˜ë³µë¬¸ íë¦„ ì„¤ê³„\n",
      "- í•µì‹¬ ê°œë…ì— í¬í•¨ëœ ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "  - ë¡œë˜ì˜ ìµœê³  ìˆœìœ„ì™€ ìµœì € ìˆœìœ„ (Lv.1)\n",
      "  - ì‹ ê³  ê²°ê³¼ ë°›ê¸° (Lv.1)\n",
      "  - ëª…ì˜ˆì˜ ì „ë‹¹ (1) (Lv.1)\n",
      "  - ê°œì¸ì •ë³´ ìˆ˜ì§‘ ìœ íš¨ê¸°ê°„ (Lv.1~2)\n",
      "  - í–„ë²„ê±° ë§Œë“¤ê¸° (Lv.1)\n",
      "\n",
      "[ê·¼ê±°]\n",
      "(1) ê³„íš | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(2) ê³„íš | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(3) ê³„íš | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(4) ìŠ¤í„°ë””ë°©í–¥ì„± | https://www.notion.so/24977d75e00880369b93e2653613eec3\n",
      "(5) ìŠ¤í„°ë””ë°©í–¥ì„± | https://www.notion.so/24977d75e00880369b93e2653613eec3\n"
     ]
    }
   ],
   "source": [
    "# --- LangChain ë²„ì „: lanchainì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ êµ¬ì¡°ë¡œ ë³€í˜•í•˜ëŠ” ì‘ì—… í•„ìš”!! ---\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "# StrOutputParser ê²½ë¡œ í˜¸í™˜\n",
    "try:\n",
    "    from langchain.schema.output_parser import StrOutputParser\n",
    "except ImportError:\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "_prompt_LC = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    (\"user\",\n",
    "     \"ì‚¬ìš©ì ì§ˆë¬¸: {query}\\n\\n\"\n",
    "     \"ë‹¤ìŒ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì •í™•íˆ ë‹µí•˜ì„¸ìš”.\\n\"\n",
    "     \"ê·¼ê±°:\\n{contexts}\\n\\n\"\n",
    "     \"ê·œì¹™:\\n\"\n",
    "     \"- ê·¼ê±°ì— ì—†ëŠ” ë‚´ìš©ì€ 'ê·¼ê±° ì—†ìŒ'ìœ¼ë¡œ í‘œì‹œ\\n\"\n",
    "     \"- í•„ìš”í•œ ê²½ìš° ëª©ë¡/í‘œë¡œ ê°„ê²°íˆ\\n\"\n",
    "     \"- ê° ì£¼ì¥ì—ëŠ” ê·¼ê±° ë²ˆí˜¸ë¥¼ ë¶™ì—¬ë¼\")\n",
    "])\n",
    "\n",
    "\n",
    "# LangChain í˜¸í™˜ ê°€ëŠ¥ OpenAI ë¶ˆëŸ¬ì˜¤ê¸° ** LangChain ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸?ë§Œ ê°€ëŠ¥í•¨ / ì œì•½ì´ ìˆëŠ”í¸(ê·¸ë˜ë„ ëŒ€ë¶€ë¶„ ëª¨ë¸ ê°€ëŠ¥)\n",
    "# LLM ì •ì˜ (ê¸°ì¡´ clientì™€ ë™ì¼í•œ ì„¤ì •)\n",
    "try:\n",
    "    LC_llm = ChatOpenAI(\n",
    "        model=MODEL_NAME,\n",
    "        api_key=API_KEY,\n",
    "        base_url=BASE_URL,\n",
    "    )\n",
    "except TypeError:  # ë²„ì „ í˜¸í™˜\n",
    "    LC_llm = ChatOpenAI(\n",
    "        model=MODEL_NAME,\n",
    "        openai_api_key=API_KEY,\n",
    "        openai_api_base=BASE_URL,\n",
    "    )\n",
    "\n",
    "# ì²´ì¸: build_prompt â†’ LLM â†’ íŒŒì„œ\n",
    "chain_A = (\n",
    "    RunnableLambda(lambda x: build_prompt(x[\"query\"], x[\"contexts\"]))\n",
    "    | LC_llm\n",
    "    | StrOutputParser() # ë¬¸ìì—´ë§Œ ê·¸ëŒ€ë¡œ Outputë‚´ëŠ” í•¨ìˆ˜\n",
    ")\n",
    "\n",
    "def _format_contexts(contexts):\n",
    "    blocks = []\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        meta = c.get(\"meta\", {}) or {}\n",
    "        title = meta.get(\"title\", \"(ì œëª©ì—†ìŒ)\")\n",
    "        section = meta.get(\"section\", \"\")\n",
    "        text = c.get(\"text\", \"\")\n",
    "        blocks.append(f\"[{i}] {title} / {section}\\n{text}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "# í•¨ìˆ˜: ask_Aì—ì„œ ì“¸ ìˆ˜ ìˆê²Œ ë˜í•‘\n",
    "def llm_answer_A(query, contexts, temperature=0.2, max_tokens=800):\n",
    "    llm_bound = LC_llm.bind(temperature=temperature,\n",
    "                           max_tokens=max_tokens,\n",
    "                           max_completion_tokens=max_tokens)\n",
    "    # ê²€ìƒ‰ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±ë¶€í„° LLM ëª¨ë¸ë¡œ ë¬¸ì¥ ìƒì„±ê¹Œì§€ Chainìœ¼ë¡œ ì—°ê²°\n",
    "    chain = (\n",
    "        {  # ì…ë ¥ ë§¤í•‘\n",
    "            \"query\": RunnablePassthrough(),\n",
    "            \"contexts\": RunnableLambda(lambda x: _format_contexts(x[\"contexts\"]))\n",
    "        }\n",
    "        | _prompt_LC      # â† ì—¬ê¸°ì„œ system/user ë©”ì‹œì§€ë¡œ ë³€í™˜\n",
    "        | llm_bound\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke({\"query\": query, \"contexts\": contexts})\n",
    "\n",
    "def embed_one(text):\n",
    "    return embed([text])[0]\n",
    "\n",
    "def ask_A(q: str, k: int = 8, n_ctx: int = 5):\n",
    "    qv = embed_one(q)\n",
    "    cands = store.search(qv, k=k) # ê²€ìƒ‰ ë‹¨ê³„ëŠ” ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€\n",
    "    contexts = cands[:n_ctx]\n",
    "    answer = llm_answer_A(q, contexts)\n",
    "    print(\"\\n[ë‹µë³€]\\n\", answer)\n",
    "    print(\"\\n[ê·¼ê±°]\")\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        print(f\"({i}) {c['meta']['title']} | {c['meta'].get('url','')}\")\n",
    "    return answer, contexts\n",
    "\n",
    "# ì˜ˆì‹œ ì‹¤í–‰:\n",
    "answer, ctx = ask_A(\"Week 3 ê³„íš\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NaqyfURmjm96",
   "metadata": {
    "id": "NaqyfURmjm96"
   },
   "source": [
    "\n",
    "---\n",
    "# Bì•ˆ: 5~6ë‹¨ê³„ LangChain êµì²´\n",
    "\n",
    "- 5)~6) **ì„ë² ë”©/ë²¡í„°ì¸ë±ìŠ¤ + QA ì²´ì¸(Aì•ˆ 6 ë‹¨ê³„)** ë¶€ë¶„ì„ LangChain ì²´ì¸ìœ¼ë¡œ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "nd_eocalj5En",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74728,
     "status": "ok",
     "timestamp": 1757701672001,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "nd_eocalj5En",
    "outputId": "a316612b-c502-4bf8-d697-5aaa05477998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ë‹µë³€]\n",
      " Week 3 ê³„íšì…ë‹ˆë‹¤.\n",
      "\n",
      "[ê·¼ê±°]\n",
      "(1) ê³„íš | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(2) ìŠ¤í„°ë””ë°©í–¥ì„± | https://www.notion.so/24977d75e00880369b93e2653613eec3\n"
     ]
    }
   ],
   "source": [
    "# --- í˜¸í™˜ ì„í¬íŠ¸ ---\n",
    "try:\n",
    "    from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "except ImportError:\n",
    "    from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "try:\n",
    "    from langchain.schema.output_parser import StrOutputParser\n",
    "except ImportError:\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- ë²¡í„°ìŠ¤í† ì–´ / ì„ë² ë”© ë˜í¼ ---\n",
    "try:\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "except ImportError:\n",
    "    from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class LegacyEmbeddings(Embeddings):\n",
    "    def __init__(self, embed_fn): self.embed_fn = embed_fn\n",
    "    def embed_documents(self, texts): return self.embed_fn(texts)\n",
    "    def embed_query(self, text): return self.embed_fn([text])[0]\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.docstore.document import Document as LCDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "# ===== 0) SYSTEM ë¶„ë¦¬ í”„ë¡¬í”„íŠ¸ =====\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "_prompt_LC = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    (\"user\",\n",
    "     \"ì‚¬ìš©ì ì§ˆë¬¸: {query}\\n\\n\"\n",
    "     \"ë‹¤ìŒ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì •í™•íˆ ë‹µí•˜ì„¸ìš”.\\n\"\n",
    "     \"ê·¼ê±°:\\n{contexts}\\n\\n\"\n",
    "     \"ê·œì¹™:\\n\"\n",
    "     \"- ê·¼ê±°ì— ì—†ëŠ” ë‚´ìš©ì€ 'ê·¼ê±° ì—†ìŒ'ìœ¼ë¡œ í‘œì‹œ\\n\"\n",
    "     \"- í•„ìš”í•œ ê²½ìš° ëª©ë¡/í‘œë¡œ ê°„ê²°íˆ\\n\"\n",
    "     \"- ê° ì£¼ì¥ì—ëŠ” ê·¼ê±° ë²ˆí˜¸ë¥¼ ë¶™ì—¬ë¼\")\n",
    "])\n",
    "\n",
    "# LLM ì´ˆê¸°í™” (ê¸°ì¡´ ëª¨ë¸/í‚¤/base_url ê·¸ëŒ€ë¡œ)\n",
    "try:\n",
    "    LC_llm = ChatOpenAI(model=MODEL_NAME, api_key=API_KEY, base_url=BASE_URL)\n",
    "except TypeError:\n",
    "    LC_llm = ChatOpenAI(model=MODEL_NAME, openai_api_key=API_KEY, openai_api_base=BASE_URL)\n",
    "\n",
    "# ===== 1) ì œëª© ë³´ì™„(í—¤ë” ì¶”ì •) =====\n",
    "def _safe_title(meta: dict, text: str):\n",
    "    t = (meta or {}).get(\"title\")\n",
    "    if isinstance(t, str) and t.strip():\n",
    "        return t.strip()\n",
    "    m = re.search(r\"^\\s*#{1,6}\\s+(.+)$\", text, flags=re.MULTILINE)\n",
    "    return (m.group(1).strip() if m else \"(ì œëª©ì—†ìŒ)\")\n",
    "\n",
    "# ===== 2) docs(LangChain Document or Notion dict) â†’ LangChain Document í‘œì¤€í™” =====\n",
    "# FAISS ë“± LangChainì˜ VectorStoreëŠ” ë°˜ë“œì‹œ Document íƒ€ì…ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ\n",
    "def _to_documents(docs_or_chunks):\n",
    "    docs = []\n",
    "    for item in docs_or_chunks:\n",
    "        if isinstance(item, LCDocument):\n",
    "            meta = dict(item.metadata or {})\n",
    "            if \"title\" not in meta:\n",
    "                meta[\"title\"] = _safe_title(meta, item.page_content)\n",
    "            if \"url\" not in meta:\n",
    "                pid = meta.get(\"page_id\") or meta.get(\"id\")\n",
    "                if isinstance(pid, str) and pid:\n",
    "                    meta[\"url\"] = f\"https://www.notion.so/{pid.replace('-','')}\"\n",
    "            docs.append(Document(page_content=item.page_content, metadata=meta))\n",
    "        elif isinstance(item, dict):\n",
    "            text = item.get(\"content_md\") or item.get(\"text\") or \"\"\n",
    "            meta = {\n",
    "                \"title\": item.get(\"title\") or \"\",\n",
    "                \"url\": item.get(\"url\") or \"\",\n",
    "                \"page_id\": item.get(\"page_id\") or item.get(\"id\"),\n",
    "                \"last_edited_time\": item.get(\"last_edited_time\"),\n",
    "            }\n",
    "            if not meta[\"title\"]:\n",
    "                meta[\"title\"] = _safe_title(meta, text)\n",
    "            if not meta[\"url\"] and meta.get(\"page_id\"):\n",
    "                meta[\"url\"] = f\"https://www.notion.so/{meta['page_id'].replace('-','')}\"\n",
    "            docs.append(Document(page_content=text, metadata=meta))\n",
    "        else:\n",
    "            docs.append(Document(page_content=str(item), metadata={}))\n",
    "    return docs\n",
    "\n",
    "# ===== 3) ì´ë¯¸ ìˆëŠ” docsë¡œ ë°”ë¡œ retriever =====\n",
    "def build_retriever_B_from_docs(docs_ready, k: int = 8):\n",
    "    emb = LegacyEmbeddings(embed)   # ê¸°ì¡´ embed([...]) ì¬ì‚¬ìš©\n",
    "    docs_std = _to_documents(docs_ready)\n",
    "    vs = FAISS.from_documents(docs_std, emb)\n",
    "    return vs.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# ===== 4) LangChain Document â†’ ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸(dict) í¬ë§· + title/url ë³´ì¥ =====\n",
    "def _contexts_from_docs(docs):\n",
    "    ctxs = []\n",
    "    for d in docs:\n",
    "        meta = dict(d.metadata or {})\n",
    "        if \"title\" not in meta or not meta[\"title\"]:\n",
    "            meta[\"title\"] = _safe_title(meta, d.page_content)\n",
    "        if \"url\" not in meta or not meta[\"url\"]:\n",
    "            pid = meta.get(\"page_id\") or meta.get(\"id\")\n",
    "            if isinstance(pid, str) and pid:\n",
    "                meta[\"url\"] = f\"https://www.notion.so/{pid.replace('-','')}\"\n",
    "        ctxs.append({\"text\": d.page_content, \"meta\": meta})\n",
    "    return ctxs\n",
    "\n",
    "# ===== 5) contexts(dict ë¦¬ìŠ¤íŠ¸) â†’ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ =====\n",
    "def _format_contexts_for_prompt(contexts):\n",
    "    blocks = []\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        meta = c.get(\"meta\", {}) or {}\n",
    "        title = meta.get(\"title\", \"(ì œëª©ì—†ìŒ)\")\n",
    "        section = meta.get(\"section\", \"\")\n",
    "        text = c.get(\"text\", \"\")\n",
    "        blocks.append(f\"[{i}] {title} / {section}\\n{text}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "# ===== 6) ì²´ì¸ ë¹Œë”: retriever â†’ (query,contexts_str) â†’ _prompt_LC â†’ LLM â†’ Parser =====\n",
    "def llm_answer_B(retriever, *, temperature: float = 0.2, max_tokens: int = 800):\n",
    "    llm_bound = LC_llm.bind(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        max_completion_tokens=max_tokens\n",
    "    )\n",
    "    chain = (\n",
    "        {\"query\": RunnablePassthrough(), \"docs\": retriever}   # ì§ˆì˜ â†’ ê²€ìƒ‰\n",
    "        | RunnableLambda(lambda x: {\n",
    "            \"query\": x[\"query\"],\n",
    "            \"contexts\": _format_contexts_for_prompt(_contexts_from_docs(x[\"docs\"]))\n",
    "        })\n",
    "        | _prompt_LC\n",
    "        | llm_bound\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "# ===== 7) ì‹¤í–‰ í—¬í¼ =====\n",
    "def ask_B(q: str, chain, retriever, n_ctx: int = 5):\n",
    "    answer = chain.invoke({\"query\": q})\n",
    "    docs_top = retriever.get_relevant_documents(q)[:n_ctx]\n",
    "    contexts = _contexts_from_docs(docs_top)\n",
    "\n",
    "    print(\"\\n[ë‹µë³€]\\n\", answer)\n",
    "    print(\"\\n[ê·¼ê±°]\")\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        print(f\"({i}) {c['meta'].get('title','(ì œëª©ì—†ìŒ)')} | {c['meta'].get('url','')}\")\n",
    "    return answer, contexts\n",
    "\n",
    "# ===== 8) ì‚¬ìš© ì˜ˆì‹œ =====\n",
    "retrieverB = build_retriever_B_from_docs(docs, k=8)  # docs: ë‹¹ì‹ ì´ ê°€ì§„ Notion dict ë¦¬ìŠ¤íŠ¸\n",
    "chainB = llm_answer_B(retrieverB, temperature=0.2, max_tokens=800)\n",
    "question = \"Week 3 ê³„íš\"\n",
    "answer, ctx = ask_B(question, chainB, retrieverB, n_ctx=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J8bOWW3iEJZo",
   "metadata": {
    "id": "J8bOWW3iEJZo"
   },
   "source": [
    "\n",
    "---\n",
    "# Cì•ˆ: Full LangChain íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "- ë°ì´í„° ë¡œë”©(~docs ìƒì„±ê¹Œì§€ ìœ ì§€) -> ì²­í‚¹ -> ì„ë² ë”©/ì¸ë±ìŠ¤ -> ê²€ìƒ‰(ë¦¬íŠ¸ë¦¬ë²„) -> QA ì²´ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ARGuIn-FTMqP",
   "metadata": {
    "id": "ARGuIn-FTMqP"
   },
   "outputs": [],
   "source": [
    "# LangChainì˜ Notion ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆëŠ”ë° ë°ì´í„°ë² ì´ìŠ¤ë§Œ ê°€ëŠ¥,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "Ly2l53zazLe3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61683,
     "status": "ok",
     "timestamp": 1757704278070,
     "user": {
      "displayName": "Subin Oh",
      "userId": "15280396810864189546"
     },
     "user_tz": -540
    },
    "id": "Ly2l53zazLe3",
    "outputId": "20e2c285-a502-488a-8a69-96bfe23d4281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ë‹µë³€]\n",
      " Week 3 ê³„íšì…ë‹ˆë‹¤.\n",
      "\n",
      "Week 3ì—ì„œëŠ” ì¡°ê±´ + êµ¬í˜„ + ì‹œë®¬ë ˆì´ì…˜ì„ ì£¼ì œë¡œ ê³µë¶€í•©ë‹ˆë‹¤. í•µì‹¬ ê°œë…ì€ ì¡°ê±´ ë¶„ê¸°ë¬¸, ê°’ ëˆ„ì , ë°˜ë³µë¬¸ íë¦„ ì„¤ê³„ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì£¼ì–´ì§„ ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "- ë¡œë˜ì˜ ìµœê³  ìˆœìœ„ì™€ ìµœì € ìˆœìœ„ (Lv.1)\n",
      "- ì‹ ê³  ê²°ê³¼ ë°›ê¸° (Lv.1)\n",
      "- ëª…ì˜ˆì˜ ì „ë‹¹ (1) (Lv.1)\n",
      "- ê°œì¸ì •ë³´ ìˆ˜ì§‘ ìœ íš¨ê¸°ê°„ (Lv.1~2)\n",
      "- í–„ë²„ê±° ë§Œë“¤ê¸° (Lv.1)\n",
      "\n",
      "ì´ ë¬¸ì œë“¤ì€ ì¡°ê±´ ë¶„ê¸°ë¬¸, ê°’ ëˆ„ì , ë°˜ë³µë¬¸ íë¦„ ì„¤ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[ê·¼ê±°]\n",
      "(1) ê³„íš | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(2) ìŠ¤í„°ë””ë°©í–¥ì„± | https://www.notion.so/24977d75e00880369b93e2653613eec3\n",
      "(3) ê³„íš | https://www.notion.so/22e77d75e00880ea9085d1410be465fb\n",
      "(4) ìŠ¤í„°ë””ë°©í–¥ì„± | https://www.notion.so/24977d75e00880369b93e2653613eec3\n",
      "(5) ìŠ¤í„°ë””ë°©í–¥ì„± | https://www.notion.so/24977d75e00880369b93e2653613eec3\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cì•ˆ: Full LangChain íŒŒì´í”„ë¼ì¸\n",
    "# ë¡œë”©(ì„ íƒ) â†’ ì²­í‚¹ â†’ ì„ë² ë”©/ì¸ë±ìŠ¤ â†’ Retriever â†’ SYSTEM ë¶„ë¦¬ í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ íŒŒì„œ\n",
    "# ===============================\n",
    "\n",
    "# ---- í˜¸í™˜ ì„í¬íŠ¸ (ë²„ì „ë³„ ê²½ë¡œ ì°¨ì´ ì²˜ë¦¬) ----\n",
    "try:\n",
    "    from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "except ImportError:\n",
    "    from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "try:\n",
    "    from langchain.schema.output_parser import StrOutputParser\n",
    "except ImportError:\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "try:\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "except ImportError:\n",
    "    from langchain.embeddings.base import Embeddings\n",
    "\n",
    "import re, uuid\n",
    "from typing import List, Iterable, Dict, Any\n",
    "\n",
    "# ---- LLM ì´ˆê¸°í™” (ê¸°ì¡´ ì„¤ì • ê·¸ëŒ€ë¡œ) ----\n",
    "try:\n",
    "    LC_llm = ChatOpenAI(model=MODEL_NAME, api_key=API_KEY, base_url=BASE_URL)\n",
    "except TypeError:\n",
    "    LC_llm = ChatOpenAI(model=MODEL_NAME, openai_api_key=API_KEY, openai_api_base=BASE_URL)\n",
    "\n",
    "# ---- SYSTEM ë¶„ë¦¬ í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´ ê·œì¹™ ìœ ì§€) ----\n",
    "_prompt_C = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    (\"user\",\n",
    "     \"ì‚¬ìš©ì ì§ˆë¬¸: {query}\\n\\n\"\n",
    "     \"ë‹¤ìŒ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì •í™•íˆ ë‹µí•˜ì„¸ìš”.\\n\"\n",
    "     \"ê·¼ê±°:\\n{contexts}\\n\\n\"\n",
    "     \"ê·œì¹™:\\n\"\n",
    "     \"- ê·¼ê±°ì— ì—†ëŠ” ë‚´ìš©ì€ 'ê·¼ê±° ì—†ìŒ'ìœ¼ë¡œ í‘œì‹œ\\n\"\n",
    "     \"- í•„ìš”í•œ ê²½ìš° ëª©ë¡/í‘œë¡œ ê°„ê²°íˆ\\n\"\n",
    "     \"- ê° ì£¼ì¥ì—ëŠ” ê·¼ê±° ë²ˆí˜¸ë¥¼ ë¶™ì—¬ë¼\")\n",
    "])\n",
    "\n",
    "# ---- ê¸°ì¡´ embed([...])ë¥¼ LangChain Embeddingsë¡œ ë˜í•‘ ----\n",
    "class LegacyEmbeddings(Embeddings):\n",
    "    def __init__(self, embed_fn): self.embed_fn = embed_fn\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.embed_fn(texts)\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_fn([text])[0]\n",
    "\n",
    "# ---- ì œëª© ë³´ì™„ (í—¤ë”ì—ì„œ ì¶”ì •) ----\n",
    "def _safe_title(meta: dict, text: str):\n",
    "    t = (meta or {}).get(\"title\")\n",
    "    if isinstance(t, str) and t.strip():\n",
    "        return t.strip()\n",
    "    m = re.search(r\"^\\s*#{1,6}\\s+(.+)$\", text, flags=re.MULTILINE)\n",
    "    return (m.group(1).strip() if m else \"(ì œëª©ì—†ìŒ)\")\n",
    "\n",
    "# ---- Notion dict â†’ LangChain Document ë³€í™˜ (docsê°€ ì´ë¯¸ ìˆìœ¼ë©´ ì´ê±¸ ì‚¬ìš©) ----\n",
    "def docs_from_notion_dicts(items: List[dict]) -> List[Document]:\n",
    "    out = []\n",
    "    for it in items:\n",
    "        text = it.get(\"content_md\") or it.get(\"text\") or \"\"\n",
    "        meta = {\n",
    "            \"title\": it.get(\"title\") or \"\",\n",
    "            \"url\": it.get(\"url\") or \"\",\n",
    "            \"page_id\": it.get(\"page_id\") or it.get(\"id\"),\n",
    "            \"last_edited_time\": it.get(\"last_edited_time\"),\n",
    "            \"section\": it.get(\"section\",\"\"),\n",
    "        }\n",
    "        if not meta[\"title\"]:\n",
    "            meta[\"title\"] = _safe_title(meta, text)\n",
    "        if not meta[\"url\"] and isinstance(meta.get(\"page_id\"), str) and meta[\"page_id\"]:\n",
    "            meta[\"url\"] = f\"https://www.notion.so/{meta['page_id'].replace('-','')}\"\n",
    "        out.append(Document(page_content=text, metadata=meta))\n",
    "    return out\n",
    "\n",
    "# ---- (ì„ íƒ) Notion DBì—ì„œ ì§ì ‘ ë¡œë”©í•˜ë ¤ë©´ ì‚¬ìš© ----\n",
    "# from langchain_community.document_loaders import NotionDBLoader\n",
    "# def _normalize_notion_id(raw: str, with_hyphens: bool = True) -> str:\n",
    "#     hex32 = \"\".join(re.findall(r\"[0-9a-fA-F]\", raw)).lower()\n",
    "#     if len(hex32) != 32: raise ValueError(f\"Notion ID ê¸¸ì´ ì˜¤ë¥˜: {raw}\")\n",
    "#     return str(uuid.UUID(hex32)) if with_hyphens else hex32\n",
    "# def load_notion_db_docs(database_id: str, notion_token: str) -> List[Document]:\n",
    "#     db_id = _normalize_notion_id(database_id, with_hyphens=True)\n",
    "#     loader = NotionDBLoader(integration_token=notion_token, database_id=db_id)\n",
    "#     docs_loaded = loader.load()\n",
    "#     out = []\n",
    "#     for d in docs_loaded:\n",
    "#         meta = dict(d.metadata or {})\n",
    "#         if \"title\" not in meta or not meta[\"title\"]:\n",
    "#             m = re.search(r\"^\\s*#{1,6}\\s+(.+)$\", d.page_content, flags=re.MULTILINE)\n",
    "#             meta[\"title\"] = (m.group(1).strip() if m else \"(ì œëª©ì—†ìŒ)\")\n",
    "#         if \"url\" not in meta or not meta[\"url\"]:\n",
    "#             pid = meta.get(\"page_id\") or meta.get(\"id\")\n",
    "#             if isinstance(pid, str) and pid:\n",
    "#                 meta[\"url\"] = f\"https://www.notion.so/{pid.replace('-','')}\"\n",
    "#         out.append(Document(page_content=d.page_content, metadata=meta))\n",
    "#     return out\n",
    "\n",
    "# ---- ì²­í‚¹ ----\n",
    "def chunk_documents(docs: List[Document], chunk_size: int = 1000, chunk_overlap: int = 150) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# ---- ì¸ë±ì‹±/ë¦¬íŠ¸ë¦¬ë²„ (FAISS) ----\n",
    "def build_vs_and_retriever_C(chunked_docs: List[Document],\n",
    "                             embed_like=None,\n",
    "                             k: int = 8,\n",
    "                             search_type: str = \"mmr\",\n",
    "                             search_kwargs: Dict[str, Any] = None):\n",
    "    embeddings = embed_like or LegacyEmbeddings(embed)  # embed([...]) ì‚¬ìš©\n",
    "    vs = FAISS.from_documents(chunked_docs, embeddings)\n",
    "    if search_kwargs is None:\n",
    "        search_kwargs = {\"k\": k, \"fetch_k\": 32, \"lambda_mult\": 0.5} if search_type==\"mmr\" else {\"k\": k}\n",
    "    retriever = vs.as_retriever(search_type=search_type, search_kwargs=search_kwargs)\n",
    "    return vs, retriever\n",
    "\n",
    "# ---- ì»¨í…ìŠ¤íŠ¸ ë³€í™˜ (LLM í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´) ----\n",
    "def _contexts_from_docs(docs: Iterable[Document]):\n",
    "    ctxs = []\n",
    "    for d in docs:\n",
    "        meta = dict(d.metadata or {})\n",
    "        if \"title\" not in meta or not meta[\"title\"]:\n",
    "            meta[\"title\"] = _safe_title(meta, d.page_content)\n",
    "        if \"url\" not in meta or not meta[\"url\"]:\n",
    "            pid = meta.get(\"page_id\") or meta.get(\"id\")\n",
    "            if isinstance(pid, str) and pid:\n",
    "                meta[\"url\"] = f\"https://www.notion.so/{pid.replace('-','')}\"\n",
    "        ctxs.append({\"text\": d.page_content, \"meta\": meta})\n",
    "    return ctxs\n",
    "\n",
    "def _format_contexts_for_prompt(contexts: List[Dict[str, Any]]) -> str:\n",
    "    blocks = []\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        meta = c.get(\"meta\", {}) or {}\n",
    "        title = meta.get(\"title\", \"(ì œëª©ì—†ìŒ)\")\n",
    "        section = meta.get(\"section\", \"\")\n",
    "        text = c.get(\"text\", \"\")\n",
    "        blocks.append(f\"[{i}] {title} / {section}\\n{text}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "# ---- QA ì²´ì¸ (Retriever â†’ Prompt â†’ LLM â†’ Parser) ----\n",
    "def build_chain_C(retriever, *, temperature: float = 0.2, max_tokens: int = 800):\n",
    "    llm_bound = LC_llm.bind(temperature=temperature, max_tokens=max_tokens, max_completion_tokens=max_tokens)\n",
    "    chain = (\n",
    "        {\"query\": RunnablePassthrough(), \"docs\": retriever}\n",
    "        | RunnableLambda(lambda x: {\n",
    "            \"query\": x[\"query\"],\n",
    "            \"contexts\": _format_contexts_for_prompt(_contexts_from_docs(x[\"docs\"]))\n",
    "        })\n",
    "        | _prompt_C\n",
    "        | llm_bound\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "def ask_C(question: str, chain, retriever, n_ctx: int = 5):\n",
    "    \"\"\"ì‹¤í–‰ + ìƒìœ„ ê·¼ê±° ì¶œë ¥ + ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "    answer = chain.invoke({\"query\": question})\n",
    "    top_docs = retriever.get_relevant_documents(question)[:n_ctx]\n",
    "    contexts = _contexts_from_docs(top_docs)\n",
    "\n",
    "    print(\"\\n[ë‹µë³€]\\n\", answer)\n",
    "    print(\"\\n[ê·¼ê±°]\")\n",
    "    for i, c in enumerate(contexts, 1):\n",
    "        print(f\"({i}) {c['meta'].get('title','(ì œëª©ì—†ìŒ)')} | {c['meta'].get('url','')}\")\n",
    "    return answer, contexts\n",
    "\n",
    "# ===============================\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "# ===============================\n",
    "# (A) ì´ë¯¸ ê°€ì§€ê³  ìˆëŠ” docs(dict ë¦¬ìŠ¤íŠ¸)ë¥¼ ì‚¬ìš©\n",
    "docs_lc = docs_from_notion_dicts(docs)           # dict â†’ LC Document\n",
    "chunked = chunk_documents(docs_lc, 1000, 150)    # ì²­í‚¹\n",
    "vsC, retrieverC = build_vs_and_retriever_C(chunked, k=8, search_type=\"mmr\")\n",
    "chainC = build_chain_C(retrieverC, temperature=0.2, max_tokens=800)\n",
    "\n",
    "question = \"Week 3 ê³„íš\"\n",
    "answer, ctx = ask_C(question, chainC, retrieverC, n_ctx=5)\n",
    "\n",
    "# (B) ë‹¤ë¥¸ ì„ë² ë”© ì“°ê³  ì‹¶ìœ¼ë©´:\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# emb_alt = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=API_KEY, base_url=BASE_URL)\n",
    "# vsC2, retrieverC2 = build_vs_and_retriever_C(chunked, embed_like=emb_alt, k=8, search_type=\"similarity\")\n",
    "# chainC2 = build_chain_C(retrieverC2)\n",
    "# answer2, ctx2 = ask_C(\"ì§ˆë¬¸\", chainC2, retrieverC2)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
